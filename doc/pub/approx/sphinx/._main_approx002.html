

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Approximation of vectors</title>
    
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="_static/sidebar.js"></script>

        <script src="http://sagecell.sagemath.org/static/jquery.min.js"></script>
        <script src="http://sagecell.sagemath.org/static/embedded_sagecell.js"></script>

        <script>sagecell.makeSagecell({inputLocation: ".sage"});</script>

        <style type="text/css">
                .sagecell .CodeMirror-scroll {
                        overflow-y: hidden;
                        overflow-x: auto;
                }
                .sagecell .CodeMirror {
                        height: auto;
                }
        </style>

    
    <link rel="top" title="Approximation of functions" href="index.html" />
    <link rel="next" title="Approximation of functions" href="._main_approx003.html" />
    <link rel="prev" title="&lt;no title&gt;" href="._main_approx001.html" />
 
  
       <style type="text/css">
         div.admonition {
           background-color: whiteSmoke;
           border: 1px solid #bababa;
         }
       </style>
      </head>
    
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="._main_approx003.html" title="Approximation of functions"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="._main_approx001.html" title="&lt;no title&gt;"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">Approximation of functions</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="approximation-of-vectors">
<span id="fem-approx-vec"></span><h1>Approximation of vectors<a class="headerlink" href="#approximation-of-vectors" title="Permalink to this headline">¶</a></h1>
<p>We shall start with introducing two fundamental methods for
determining the coefficients <span class="math">\(c_i\)</span> in <a class="reference internal" href="._main_approx001.html#eq-fem-u"><span>(2.1)</span></a> and illustrate
the methods on approximation of vectors, because vectors in vector
spaces give a more intuitive understanding than starting directly
with approximation of functions in function spaces.
The extension from vectors to functions will be trivial as soon as
the fundamental ideas are understood.</p>
<p>The first method of approximation is called the <em>least squares method</em>
and consists in finding <span class="math">\(c_i\)</span> such that the difference <span class="math">\(u-f\)</span>, measured
in some norm, is minimized. That is, we aim at finding the best
approximation <span class="math">\(u\)</span> to <span class="math">\(f\)</span> (in some norm). The second method is not
as intuitive: we find <span class="math">\(u\)</span> such that the error <span class="math">\(u-f\)</span> is orthogonal to
the space where we seek <span class="math">\(u\)</span>. This is known as <em>projection</em>, or
we may also call it a <em>Galerkin method</em>.
When approximating vectors and functions, the two methods are
equivalent, but this is no longer the case when applying the
principles to differential equations.</p>
<div class="section" id="approximation-of-planar-vectors">
<span id="fem-approx-vec-plane"></span><h2>Approximation of planar vectors<a class="headerlink" href="#approximation-of-planar-vectors" title="Permalink to this headline">¶</a></h2>
<p id="index-0">Suppose we have given a vector <span class="math">\(\boldsymbol{f} = (3,5)\)</span> in the <span class="math">\(xy\)</span> plane
and that we want to approximate this vector by a vector aligned
in the direction of the vector <span class="math">\((a,b)\)</span>. Figure <a class="reference internal" href="#fem-approx-vec-plane-fig"><span>Approximation of a two-dimensional vector by a one-dimensional vector</span></a>
depicts the situation.</p>
<div class="figure" id="id1">
<span id="fem-approx-vec-plane-fig"></span><a class="reference internal image-reference" href="_images/vecapprox_plane.png"><img alt="_images/vecapprox_plane.png" src="_images/vecapprox_plane.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-text"><em>Approximation of a two-dimensional vector by a one-dimensional vector</em></span></p>
</div>
<p>We introduce the vector space <span class="math">\(V\)</span>
spanned by the vector <span class="math">\(\boldsymbol{\psi}_0=(a,b)\)</span>:</p>
<div class="math">
\[V = \mbox{span}\,\{ \boldsymbol{\psi}_0\}{\thinspace .}\]</div>
<p>We say that <span class="math">\(\boldsymbol{\psi}_0\)</span> is a basis vector in the space <span class="math">\(V\)</span>.
Our aim is to find the vector <span class="math">\(\boldsymbol{u} = c_0\boldsymbol{\psi}_0\in V\)</span> which best approximates
the given vector <span class="math">\(\boldsymbol{f} = (3,5)\)</span>. A reasonable criterion for a best
approximation could be to minimize the length of the difference between
the approximate <span class="math">\(\boldsymbol{u}\)</span> and the given <span class="math">\(\boldsymbol{f}\)</span>. The difference, or error
<span class="math">\(\boldsymbol{e} = \boldsymbol{f} -\boldsymbol{u}\)</span>, has its length given by the <em>norm</em></p>
<div class="math">
\[||\boldsymbol{e}|| = (\boldsymbol{e},\boldsymbol{e})^{\frac{1}{2}},\]</div>
<p>where <span class="math">\((\boldsymbol{e},\boldsymbol{e})\)</span> is the <em>inner product</em> of <span class="math">\(\boldsymbol{e}\)</span> and itself. The inner
product, also called <em>scalar product</em> or <em>dot product</em>, of two vectors
<span class="math">\(\boldsymbol{u}=(u_0,u_1)\)</span> and <span class="math">\(\boldsymbol{v} =(v_0,v_1)\)</span> is defined as</p>
<div class="math">
\[(\boldsymbol{u}, \boldsymbol{v}) = u_0v_0 + u_1v_1{\thinspace .}\]</div>
<p><strong>Remark 1.</strong>
We should point out that we use the notation
<span class="math">\((\cdot,\cdot)\)</span> for two different things: <span class="math">\((a,b)\)</span> for scalar
quantities <span class="math">\(a\)</span> and <span class="math">\(b\)</span> means the vector starting in the origin and
ending in the point <span class="math">\((a,b)\)</span>, while <span class="math">\((\boldsymbol{u},\boldsymbol{v})\)</span> with vectors <span class="math">\(\boldsymbol{u}\)</span> and
<span class="math">\(\boldsymbol{v}\)</span> means the inner product of these vectors.  Since vectors are here
written in boldface font there should be no confusion.  We may add
that the norm associated with this inner product is the usual
Eucledian length of a vector.</p>
<p><strong>Remark 2.</strong>
It might be wise to refresh some basic linear algebra by consulting a
textbook.  <a class="reference internal" href="._main_approx011.html#fem-approx-exer-linalg1"><span>Exercise 1: Linear algebra refresher I</span></a> and
<a class="reference internal" href="._main_approx011.html#fem-approx-exer-linalg2"><span>Exercise 2: Linear algebra refresher II</span></a> suggest specific tasks to regain
familiarity with fundamental operations on inner product vector
spaces.</p>
<div class="section" id="the-least-squares-method-1">
<span id="index-1"></span><h3>The least squares method  (1)<a class="headerlink" href="#the-least-squares-method-1" title="Permalink to this headline">¶</a></h3>
<p>We now want to find <span class="math">\(c_0\)</span> such that it minimizes <span class="math">\(||\boldsymbol{e}||\)</span>. The algebra
is simplified if we minimize the square of the norm, <span class="math">\(||\boldsymbol{e}||^2 = (\boldsymbol{e}, \boldsymbol{e})\)</span>,
instead of the norm itself.
Define the function</p>
<div class="math">
\[E(c_0) = (\boldsymbol{e},\boldsymbol{e}) = (\boldsymbol{f} - c_0\boldsymbol{\psi}_0, \boldsymbol{f} - c_0\boldsymbol{\psi}_0)
{\thinspace .}\]</div>
<p>We can rewrite the expressions of the right-hand side in a more
convenient form for further work:</p>
<div class="math" id="equation-fem:vec:E">
<span id="eq-fem-vec-e"></span><span class="eqno">(1)</span>\[     E(c_0) = (\boldsymbol{f},\boldsymbol{f}) - 2c_0(\boldsymbol{f},\boldsymbol{\psi}_0) + c_0^2(\boldsymbol{\psi}_0,\boldsymbol{\psi}_0){\thinspace .}\]</div>
<p>The rewrite results from using the following fundamental rules for inner
product spaces:</p>
<div class="math" id="equation-fem:vec:rule:scalarmult">
<span id="eq-fem-vec-rule-scalarmult"></span><span class="eqno">(2)</span>\[     (\alpha\boldsymbol{u},\boldsymbol{v})=\alpha(\boldsymbol{u},\boldsymbol{v}),\quad \alpha\in\mathbb{R},\]</div>
<div class="math" id="equation-fem:vec:rule:sum">
<span id="eq-fem-vec-rule-sum"></span><span class="eqno">(3)</span>\[     (\boldsymbol{u} +\boldsymbol{v},\boldsymbol{w}) = (\boldsymbol{u},\boldsymbol{w}) + (\boldsymbol{v}, \boldsymbol{w}),\]</div>
<div class="math" id="equation-fem:vec:rule:symmetry">
<span id="eq-fem-vec-rule-symmetry"></span><span class="eqno">(4)</span>\[     (\boldsymbol{u}, \boldsymbol{v}) = (\boldsymbol{v}, \boldsymbol{u}){\thinspace .}\]</div>
<p>Minimizing <span class="math">\(E(c_0)\)</span> implies finding <span class="math">\(c_0\)</span> such that</p>
<div class="math">
\[\frac{\partial E}{\partial c_0} = 0{\thinspace .}\]</div>
<p>Differentiating <a href="#equation-fem:vec:E">(1)</a> with respect to <span class="math">\(c_0\)</span> gives</p>
<div class="math" id="equation-fem:vec:dEdc0:v1">
<span id="eq-fem-vec-dedc0-v1"></span><span class="eqno">(5)</span>\[     \frac{\partial E}{\partial c_0} = -2(\boldsymbol{f},\boldsymbol{\psi}_0) + 2c_0 (\boldsymbol{\psi}_0,\boldsymbol{\psi}_0)
     {\thinspace .}\]</div>
<p>Setting the above expression equal to zero and solving for <span class="math">\(c_0\)</span> gives</p>
<div class="math" id="equation-fem:vec:c0">
<span id="eq-fem-vec-c0"></span><span class="eqno">(6)</span>\[     c_0 = \frac{(\boldsymbol{f},\boldsymbol{\psi}_0)}{(\boldsymbol{\psi}_0,\boldsymbol{\psi}_0)},\]</div>
<p>which in the present case with <span class="math">\(\boldsymbol{\psi}_0=(a,b)\)</span> results in</p>
<div class="math">
\[c_0 = \frac{3a + 5b}{a^2 + b^2}{\thinspace .}\]</div>
<p>For later, it is worth mentioning that setting
the key equation <a href="#equation-fem:vec:dEdc0:v1">(5)</a> to zero can be rewritten
as</p>
<div class="math">
\[(\boldsymbol{f}-c0\boldsymbol{\psi}_0,\boldsymbol{\psi}_0) = 0,\]</div>
<p>or</p>
<div class="math" id="equation-fem:vec:dEdc0:Galerkin">
<span id="eq-fem-vec-dedc0-galerkin"></span><span class="eqno">(7)</span>\[     (\boldsymbol{e}, \boldsymbol{\psi}_0) = 0
     {\thinspace .}\]</div>
<span class="target" id="index-2"></span></div>
<div class="section" id="the-projection-method">
<span id="index-3"></span><h3>The projection method<a class="headerlink" href="#the-projection-method" title="Permalink to this headline">¶</a></h3>
<p>We shall now show that minimizing <span class="math">\(||\boldsymbol{e}||^2\)</span> implies that <span class="math">\(\boldsymbol{e}\)</span> is
orthogonal to <em>any</em> vector <span class="math">\(\boldsymbol{v}\)</span> in the space <span class="math">\(V\)</span>. This result is
visually quite clear from Figure <a class="reference internal" href="#fem-approx-vec-plane-fig"><span>Approximation of a two-dimensional vector by a one-dimensional vector</span></a> (think of
other vectors along the line <span class="math">\((a,b)\)</span>: all of them will lead to
a larger distance between the approximation and <span class="math">\(\boldsymbol{f}\)</span>).
To see this result mathematically, we
express any <span class="math">\(\boldsymbol{v}\in V\)</span> as <span class="math">\(\boldsymbol{v}=s\boldsymbol{\psi}_0\)</span> for any scalar parameter <span class="math">\(s\)</span>,
recall that two vectors are orthogonal when their inner product vanishes,
and calculate the inner product</p>
<div class="math">
\[\begin{split}(\boldsymbol{e}, s\boldsymbol{\psi}_0) &amp;= (\boldsymbol{f} - c_0\boldsymbol{\psi}_0, s\boldsymbol{\psi}_0)\\
&amp;= (\boldsymbol{f},s\boldsymbol{\psi}_0) - (c_0\boldsymbol{\psi}_0, s\boldsymbol{\psi}_0)\\
&amp;= s(\boldsymbol{f},\boldsymbol{\psi}_0) - sc_0(\boldsymbol{\psi}_0, \boldsymbol{\psi}_0)\\
&amp;= s(\boldsymbol{f},\boldsymbol{\psi}_0) - s\frac{(\boldsymbol{f},\boldsymbol{\psi}_0)}{(\boldsymbol{\psi}_0,\boldsymbol{\psi}_0)}(\boldsymbol{\psi}_0,\boldsymbol{\psi}_0)\\
&amp;= s\left( (\boldsymbol{f},\boldsymbol{\psi}_0) - (\boldsymbol{f},\boldsymbol{\psi}_0)\right)\\
&amp;=0{\thinspace .}\end{split}\]</div>
<p>Therefore, instead of minimizing the square of the norm, we could
demand that <span class="math">\(\boldsymbol{e}\)</span> is orthogonal to any vector in <span class="math">\(V\)</span>.
This method is known as <em>projection</em>, because it is the same as
projecting the vector onto the subspace.
(The approach can also be referred to as a Galerkin method as
explained at the end of the section <a class="reference internal" href="#fem-approx-vec-np1dim"><span>Approximation of general vectors</span></a>.)</p>
<p>Mathematically the projection method is stated
by the equation</p>
<div class="math" id="equation-fem:vec:Galerkin1">
<span id="eq-fem-vec-galerkin1"></span><span class="eqno">(8)</span>\[     (\boldsymbol{e}, \boldsymbol{v}) = 0,\quad\forall\boldsymbol{v}\in V{\thinspace .}\]</div>
<p>An arbitrary <span class="math">\(\boldsymbol{v}\in V\)</span> can be expressed as
<span class="math">\(s\boldsymbol{\psi}_0\)</span>, <span class="math">\(s\in\mathbb{R}\)</span>, and therefore
<a href="#equation-fem:vec:Galerkin1">(8)</a> implies</p>
<div class="math">
\[(\boldsymbol{e},s\boldsymbol{\psi}_0) = s(\boldsymbol{e}, \boldsymbol{\psi}_0) = 0,\]</div>
<p>which means that the error must be orthogonal to the basis vector in
the space <span class="math">\(V\)</span>:</p>
<div class="math">
\[(\boldsymbol{e}, \boldsymbol{\psi}_0)=0\quad\hbox{or}\quad
(\boldsymbol{f} - c_0\boldsymbol{\psi}_0, \boldsymbol{\psi}_0)=0
{\thinspace .}\]</div>
<p>The latter equation gives <a href="#equation-fem:vec:c0">(6)</a> and it
also arose from least squares computations in
<a href="#equation-fem:vec:dEdc0:Galerkin">(7)</a>.</p>
</div>
</div>
<div class="section" id="approximation-of-general-vectors">
<span id="fem-approx-vec-np1dim"></span><h2>Approximation of general vectors<a class="headerlink" href="#approximation-of-general-vectors" title="Permalink to this headline">¶</a></h2>
<p id="index-4">Let us generalize the vector approximation from the previous section
to vectors in spaces with arbitrary dimension. Given some vector <span class="math">\(\boldsymbol{f}\)</span>,
we want to find the best approximation to this vector in
the space</p>
<div class="math">
\[V = \hbox{span}\,\{\boldsymbol{\psi}_0,\ldots,\boldsymbol{\psi}_N\}
{\thinspace .}\]</div>
<p>We assume that the <em>basis vectors</em> <span class="math">\(\boldsymbol{\psi}_0,\ldots,\boldsymbol{\psi}_N\)</span> are
linearly independent so that none of them are redundant and
the space has dimension <span class="math">\(N+1\)</span>.
Any vector <span class="math">\(\boldsymbol{u}\in V\)</span> can be written as a linear combination
of the basis vectors,</p>
<div class="math">
\[\boldsymbol{u} = \sum_{j=0}^N c_j\boldsymbol{\psi}_j,\]</div>
<p>where <span class="math">\(c_j\in\mathbb{R}\)</span> are scalar coefficients to be determined.</p>
<div class="section" id="the-least-squares-method-2">
<h3>The least squares method  (2)<a class="headerlink" href="#the-least-squares-method-2" title="Permalink to this headline">¶</a></h3>
<p>Now we want to find <span class="math">\(c_0,\ldots,c_N\)</span>, such that <span class="math">\(\boldsymbol{u}\)</span> is the best
approximation to <span class="math">\(\boldsymbol{f}\)</span> in the sense that the distance (error)
<span class="math">\(\boldsymbol{e} = \boldsymbol{f} - \boldsymbol{u}\)</span> is minimized. Again, we define
the squared distance as a function of the free parameters
<span class="math">\(c_0,\ldots,c_N\)</span>,</p>
<div class="math">
\[E(c_0,\ldots,c_N) = (\boldsymbol{e},\boldsymbol{e}) = (\boldsymbol{f} -\sum_jc_j\boldsymbol{\psi}_j,\boldsymbol{f} -\sum_jc_j\boldsymbol{\psi}_j)
\nonumber\]</div>
<div class="math" id="equation-fem:vec:genE">
<span id="eq-fem-vec-gene"></span><span class="eqno">(9)</span>\[     = (\boldsymbol{f},\boldsymbol{f}) - 2\sum_{j=0}^N c_j(\boldsymbol{f},\boldsymbol{\psi}_j) +
     \sum_{p=0}^N\sum_{q=0}^N c_pc_q(\boldsymbol{\psi}_p,\boldsymbol{\psi}_q){\thinspace .}\]</div>
<p>Minimizing this <span class="math">\(E\)</span> with respect to the independent variables
<span class="math">\(c_0,\ldots,c_N\)</span> is obtained by requiring</p>
<div class="math">
\[\frac{\partial E}{\partial c_i} = 0,\quad i=0,\ldots,N
{\thinspace .}\]</div>
<p>The second term in <a href="#equation-fem:vec:genE">(9)</a> is differentiated as follows:</p>
<div class="math">
\[\frac{\partial}{\partial c_i}
\sum_{j=0}^N c_j(\boldsymbol{f},\boldsymbol{\psi}_j) = (\boldsymbol{f},\boldsymbol{\psi}_i),\]</div>
<p>since the expression to be differentiated is a sum and only one term,
<span class="math">\(c_i(\boldsymbol{f},\boldsymbol{\psi}_i)\)</span>,
contains <span class="math">\(c_i\)</span> and this term is linear in <span class="math">\(c_i\)</span>.
To understand this differentiation in detail, write out the sum specifically for,
e.g, <span class="math">\(N=3\)</span> and <span class="math">\(i=1\)</span>.</p>
<p>The last term in <a href="#equation-fem:vec:genE">(9)</a>
is more tedious to differentiate. We start with</p>
<div class="math">
\[\frac{\partial}{\partial c_i}
c_pc_q =
\left\lbrace\begin{array}{ll}
0,  \hbox{ if } p\neq i\hbox{ and } q\neq i,\]</div>
<div class="math">
\[c_q,  \hbox{ if } p=i\hbox{ and } q\neq i,\]</div>
<div class="math">
\[c_p,  \hbox{ if } p\neq i\hbox{ and } q=i,\]</div>
<div class="math">
\[2c_i,  \hbox{ if } p=q= i,\]</div>
<div class="math">
\[\end{array}\right.\]</div>
<p>Then</p>
<div class="math">
\[\frac{\partial}{\partial c_i}
\sum_{p=0}^N\sum_{q=0}^N c_pc_q(\boldsymbol{\psi}_p,\boldsymbol{\psi}_q)
= \sum_{p=0, p\neq i}^N c_p(\boldsymbol{\psi}_p,\boldsymbol{\psi}_i)
+ \sum_{q=0, q\neq i}^N c_q(\boldsymbol{\psi}_q,\boldsymbol{\psi}_i)
+2c_i(\boldsymbol{\psi}_i,\boldsymbol{\psi}_i){\thinspace .}\]</div>
<p>The last term can be included in the other two sums, resulting in</p>
<div class="math">
\[\frac{\partial}{\partial c_i}
\sum_{p=0}^N\sum_{q=0}^N c_pc_q(\boldsymbol{\psi}_p,\boldsymbol{\psi}_q)
= 2\sum_{j=0}^N c_i(\boldsymbol{\psi}_j,\boldsymbol{\psi}_i){\thinspace .}\]</div>
<p>It then follows that setting</p>
<div class="math">
\[\frac{\partial E}{\partial c_i} = 0,\quad i=0,\ldots,N,\]</div>
<p>leads to a linear system
for <span class="math">\(c_0,\ldots,c_N\)</span>:</p>
<div class="math" id="equation-fem:approx:vec:Np1dim:eqsys">
<span id="eq-fem-approx-vec-np1dim-eqsys"></span><span class="eqno">(10)</span>\[     \sum_{j=0}^N A_{i,j} c_j = b_i, \quad i=0,\ldots,N,\]</div>
<p>where</p>
<div class="math">
\[A_{i,j} = (\boldsymbol{\psi}_i,\boldsymbol{\psi}_j),\]</div>
<div class="math">
\[b_i = (\boldsymbol{\psi}_i, \boldsymbol{f}){\thinspace .}\]</div>
<p>We have changed the order of the two vectors in the inner
product according to <a href="#equation-fem:vec:rule:symmetry">(4)</a>:</p>
<div class="math">
\[A_{i,j} = (\boldsymbol{\psi}_j,\boldsymbol{\psi}_i) = (\boldsymbol{\psi}_i,\boldsymbol{\psi}_j),\]</div>
<p>simply because the sequence <span class="math">\(i\)</span>-$j$ looks more aesthetic.</p>
</div>
<div class="section" id="the-galerkin-or-projection-method">
<h3>The Galerkin or projection method<a class="headerlink" href="#the-galerkin-or-projection-method" title="Permalink to this headline">¶</a></h3>
<p>In analogy with the &#8220;one-dimensional&#8221; example in
the section <a class="reference internal" href="#fem-approx-vec-plane"><span>Approximation of planar vectors</span></a>, it holds also here in the general
case that minimizing the distance
(error) <span class="math">\(\boldsymbol{e}\)</span> is equivalent to demanding that <span class="math">\(\boldsymbol{e}\)</span> is orthogonal to
all <span class="math">\(\boldsymbol{v}\in V\)</span>:</p>
<span class="target" id="index-5"></span><div class="math" id="equation-fem:approx:vec:Np1dim:Galerkin">
<span id="eq-fem-approx-vec-np1dim-galerkin"></span><span id="index-6"></span><span class="eqno">(11)</span>\[     (\boldsymbol{e},\boldsymbol{v})=0,\quad \forall\boldsymbol{v}\in V{\thinspace .}\]</div>
<p>Since any <span class="math">\(\boldsymbol{v}\in V\)</span> can be written as <span class="math">\(\boldsymbol{v} =\sum_{i=0}^N c_i\boldsymbol{\psi}_i\)</span>,
the statement <a href="#equation-fem:approx:vec:Np1dim:Galerkin">(11)</a> is equivalent to
saying that</p>
<div class="math">
\[(\boldsymbol{e}, \sum_{i=0}^N c_i\boldsymbol{\psi}_i) = 0,\]</div>
<p>for any choice of coefficients <span class="math">\(c_0,\ldots,c_N\)</span>.
The latter equation can be rewritten as</p>
<div class="math">
\[\sum_{i=0}^N c_i (\boldsymbol{e},\boldsymbol{\psi}_i) =0{\thinspace .}\]</div>
<p>If this is to hold for arbitrary values of <span class="math">\(c_0,\ldots,c_N\)</span>
we must require that each term in the sum vanishes,</p>
<div class="math" id="equation-fem:approx:vec:Np1dim:Galerkin0">
<span id="eq-fem-approx-vec-np1dim-galerkin0"></span><span class="eqno">(12)</span>\[     (\boldsymbol{e},\boldsymbol{\psi}_i)=0,\quad i=0,\ldots,N{\thinspace .}\]</div>
<p>These <span class="math">\(N+1\)</span> equations result in the same linear system as
<a href="#equation-fem:approx:vec:Np1dim:eqsys">(10)</a>:</p>
<div class="math">
\[(\boldsymbol{f} - \sum_{j=0}^N c_j\boldsymbol{\psi}_j, \boldsymbol{\psi}_i) = (\boldsymbol{f}, \boldsymbol{\psi}_i) - \sum_{j\in{\mathcal{I}_s}}
(\boldsymbol{\psi}_i,\boldsymbol{\psi}_j)c_j = 0,\]</div>
<p>and hence</p>
<div class="math">
\[\sum_{j=0}^N (\boldsymbol{\psi}_i,\boldsymbol{\psi}_j)c_j = (\boldsymbol{f}, \boldsymbol{\psi}_i),\quad i=0,\ldots, N
{\thinspace .}\]</div>
<p>So, instead of differentiating the
<span class="math">\(E(c_0,\ldots,c_N)\)</span> function, we could simply use
<a href="#equation-fem:approx:vec:Np1dim:Galerkin">(11)</a> as the principle for
determining <span class="math">\(c_0,\ldots,c_N\)</span>, resulting in the <span class="math">\(N+1\)</span>
equations <a href="#equation-fem:approx:vec:Np1dim:Galerkin0">(12)</a>.</p>
<p>The names <em>least squares method</em> or <em>least squares approximation</em>
are natural since the calculations consists of
minimizing <span class="math">\(||\boldsymbol{e}||^2\)</span>, and <span class="math">\(||\boldsymbol{e}||^2\)</span> is a sum of squares
of differences between the components in <span class="math">\(\boldsymbol{f}\)</span> and <span class="math">\(\boldsymbol{u}\)</span>.
We find <span class="math">\(\boldsymbol{u}\)</span> such that this sum of squares is minimized.</p>
<p>The principle <a href="#equation-fem:approx:vec:Np1dim:Galerkin">(11)</a>,
or the equivalent form <a href="#equation-fem:approx:vec:Np1dim:Galerkin0">(12)</a>,
is known as <em>projection</em>. Almost the same mathematical idea
was used by the Russian mathematician <a class="reference external" href="http://en.wikipedia.org/wiki/Boris_Galerkin">Boris Galerkin</a> to solve
differential equations, resulting in what is widely known as
<em>Galerkin&#8217;s method</em>.</p>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <center>
            <p class="logo"><a href="http://cbc.simula.no/" title="Go to Center for Biomedical Computing">
              <img class="logo" src="_static/cbc_logo.png" alt="Logo"/>
            </a></p>
            </center>
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Approximation of vectors</a><ul>
<li><a class="reference internal" href="#approximation-of-planar-vectors">Approximation of planar vectors</a><ul>
<li><a class="reference internal" href="#the-least-squares-method-1">The least squares method  (1)</a></li>
<li><a class="reference internal" href="#the-projection-method">The projection method</a></li>
</ul>
</li>
<li><a class="reference internal" href="#approximation-of-general-vectors">Approximation of general vectors</a><ul>
<li><a class="reference internal" href="#the-least-squares-method-2">The least squares method  (2)</a></li>
<li><a class="reference internal" href="#the-galerkin-or-projection-method">The Galerkin or projection method</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="._main_approx001.html"
                        title="previous chapter">&lt;no title&gt;</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="._main_approx003.html"
                        title="next chapter">Approximation of functions</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/._main_approx002.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" size="18" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="._main_approx003.html" title="Approximation of functions"
             >next</a> |</li>
        <li class="right" >
          <a href="._main_approx001.html" title="&lt;no title&gt;"
             >previous</a> |</li>
        <li><a href="index.html">Approximation of functions</a> &raquo;</li> 
      </ul>
    </div>
<div class="wrapper">
  <div class="footer">
  <a href="http://cbc.simula.no"><img src="_static/cbc_banner.png" width="100%"><a>
  </div>
</div>

  </body>
</html>