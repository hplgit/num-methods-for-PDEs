<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="description" content="Introduction to computing with finite difference methods">
<meta name="keywords" content="decay ODE,exponential decay,mesh,grid,mesh function,finite differences,forward difference,finite differences forward,difference equation,discrete equation,algebraic equation,finite difference scheme,Forward Euler scheme,backward difference,finite differences backward,backward scheme, 1-step,Backward Euler scheme,Crank-Nicolson scheme,centered difference,finite differences centered,averaging arithmetic,weighted average,theta-rule,$\theta$-rule,finite difference operator notation,operator notation, finite differences,directory,folder,doc strings,printf format,format string syntax (Python),plotting curves,visualizing curves,representative (mesh function),array arithmetics,array computing,continuous function norms,norm continuous,discrete function norms,mesh function norms,norm discrete (mesh function),error norms,scalar computing,PNG plot,PDF plot,EPS plot,viewing graphics files,cropping images,stability,amplification factor,A-stable methods,L-stable methods,error amplification factor,error global,consistency,stability,convergence,lambda functions,method of manufactured solutions,MMS (method of manufactured solutions),convergence rate,verification,implicit schemes,explicit schemes,theta-rule,$\theta$-rule,backward scheme, 2-step,BDF2 scheme,Leapfrog scheme,Leapfrog scheme, filtered,Heun's method,Runge-Kutta, 2nd-order method,Taylor-series methods (for ODEs),Adams-Bashforth scheme, 2nd-order,Adams-Bashforth scheme, 3rd order,Runge-Kutta, 4th-order method,RK4,adaptive time stepping,Dormand-Prince Runge-Kutta 4-5 method,population dynamics,logistic model,radioactive decay,terminal velocity,geometric mean,averaging geometric,scaling">

<title>Introduction to computing with finite difference methods</title>


<link href="https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_styles/style_solarized_box/css/solarized_light_code.css" rel="stylesheet" type="text/css" title="light"/>
<script src="https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_styles/style_solarized_box/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<link href="http://thomasf.github.io/solarized-css/solarized-light.min.css" rel="stylesheet">
<style type="text/css">
h1 {color: #b58900;}  /* yellow */
/* h1 {color: #cb4b16;}  orange */
/* h1 {color: #d33682;}  magenta, the original choice of thomasf */
code { padding: 0px; background-color: inherit; }
pre {
  border: 0pt solid #93a1a1;
  box-shadow: none;
}
.alert-text-small   { font-size: 80%;  }
.alert-text-large   { font-size: 130%; }
.alert-text-normal  { font-size: 90%;  }
.alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:1px solid #93a1a1;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  color: #555;
  background-color: #eee8d5;
  background-position: 10px 5px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 55px;
  width: 75%;
 }
.alert-block {padding-top:14px; padding-bottom:14px}
.alert-block > p, .alert-block > ul {margin-bottom:1em}
.alert li {margin-top: 1em}
.alert-block p+p {margin-top:5px}
.alert-notice { background-image: url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_yellow_notice.png); }
.alert-summary  { background-image:url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_yellow_summary.png); }
.alert-warning { background-image: url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_yellow_warning.png); }
.alert-question {background-image:url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_yellow_question.png); }

div { text-align: justify; text-justify: inter-word; }
</style>


</head>

<!-- tocinfo
{'highest level': 1,
 'sections': [(' Finite difference methods  ',
               1,
               'decay:basics',
               'decay:basics'),
              (' A basic model for exponential decay ',
               2,
               'decay:model',
               'decay:model'),
              (' The Forward Euler scheme ',
               2,
               'decay:schemes:FE',
               'decay:schemes:FE'),
              (' Step 1: Discretizing the domain ', 3, None, '___sec3'),
              (' Step 2: Fulfilling the equation at discrete time points ',
               3,
               None,
               '___sec4'),
              (' Step 3: Replacing derivatives by finite differences ',
               3,
               None,
               '___sec5'),
              (' Step 4: Formulating a recursive algorithm ',
               3,
               None,
               '___sec6'),
              (' The Backward Euler scheme ',
               2,
               'decay:schemes:BE',
               'decay:schemes:BE'),
              (' The Crank-Nicolson scheme ',
               2,
               'decay:schemes:CN',
               'decay:schemes:CN'),
              (' The unifying $\\theta$-rule ',
               2,
               'decay:schemes:theta',
               'decay:schemes:theta'),
              (' Constant time step ', 2, None, '___sec10'),
              (' Compact operator notation for finite differences ',
               2,
               'decay:fd:op',
               'decay:fd:op'),
              (' Implementation ', 1, 'decay:impl1', 'decay:impl1'),
              (' Making a solver function ', 2, 'decay:py1', 'decay:py1'),
              (' Function for computing the numerical solution ',
               3,
               None,
               '___sec14'),
              (' Integer division ', 3, None, '___sec15'),
              (' Doc strings ', 3, None, '___sec16'),
              (' Formatting of numbers ', 3, None, '___sec17'),
              (' Running the program ', 3, None, '___sec18'),
              (' Plotting the solution ', 3, None, '___sec19'),
              (' Verifying the implementation ', 2, None, '___sec20'),
              (' Running a few algorithmic steps by hand ',
               3,
               None,
               '___sec21'),
              (' Computing the numerical error as a mesh function ',
               2,
               'decay:computing:error',
               'decay:computing:error'),
              (' Computing the norm of the numerical error ',
               2,
               'decay:computing:error:norm',
               'decay:computing:error:norm'),
              (' Scalar computing ', 3, None, '___sec24'),
              (' Experiments with computing and plotting ',
               2,
               None,
               '___sec25'),
              (' Combining plot files ', 3, None, '___sec26'),
              (' Plotting with SciTools ', 3, None, '___sec27'),
              (' Memory-saving implementation ', 2, None, '___sec28'),
              (' Exercises ', 1, None, '___sec29'),
              (' Exercise 1: Differentiate a function ',
               2,
               'decay:exer:dudt',
               'decay:exer:dudt'),
              (' Exercise 2: Experiment with integer division ',
               2,
               'decay:exer:intdiv',
               'decay:exer:intdiv'),
              (' Exercise 3: Experiment with wrong computations ',
               2,
               'decay:exer:decay1err',
               'decay:exer:decay1err'),
              (' Exercise 4: Plot the error function ',
               2,
               'decay:exer:plot:error',
               'decay:exer:plot:error'),
              (' Exercise 5: Change formatting of numbers and debug ',
               2,
               'decay:exer:inexact:output',
               'decay:exer:inexact:output'),
              (' Analysis of finite difference equations ',
               1,
               'decay:analysis',
               'decay:analysis'),
              (' Experimental investigation of oscillatory solutions ',
               2,
               None,
               '___sec36'),
              (' Exact numerical solution ', 2, None, '___sec37'),
              (' Stability ', 2, None, '___sec38'),
              (' Comparing amplification factors ', 2, None, '___sec39'),
              (' Series expansion of amplification factors ',
               2,
               None,
               '___sec40'),
              (' The fraction of numerical and exact amplification factors ',
               2,
               None,
               '___sec41'),
              (' The global error at a point ',
               2,
               'decay:analysis:gobal:error',
               'decay:analysis:gobal:error'),
              (' Integrated errors ', 2, None, '___sec43'),
              (' Truncation error ', 2, None, '___sec44'),
              (' Consistency, stability, and convergence ',
               2,
               None,
               '___sec45'),
              (' Exercises ', 1, None, '___sec46'),
              (' Exercise 6: Visualize the accuracy of finite differences ',
               2,
               'decay:analysis:exer:fd:exp:plot',
               'decay:analysis:exer:fd:exp:plot'),
              (' Exercise 7: Explore the $\\theta$-rule for exponential growth ',
               2,
               'decay:analysis:exer:growth',
               'decay:analysis:exer:growth'),
              (' Model extensions ', 1, None, '___sec49'),
              (' Generalization: including a variable coefficient ',
               2,
               None,
               '___sec50'),
              (' Generalization: including a source term ',
               2,
               'decay:source',
               'decay:source'),
              (' Schemes ', 3, None, '___sec52'),
              (' Implementation of the generalized model problem ',
               2,
               'decay:general',
               'decay:general'),
              (' Deriving the $\\theta$-rule formula ', 3, None, '___sec54'),
              (' The Python code ', 3, None, '___sec55'),
              (' Coding of variable coefficients ', 3, None, '___sec56'),
              (' Verifying a constant solution ',
               2,
               'decay:verify:trivial',
               'decay:verify:trivial'),
              (' Verification via manufactured solutions ',
               2,
               'decay:MMS',
               'decay:MMS'),
              (' Computing convergence rates ',
               2,
               'decay:convergence:rate',
               'decay:convergence:rate'),
              (' Estimating $r$ ', 3, None, '___sec60'),
              (' Implementation ', 3, None, '___sec61'),
              (' Extension to systems of ODEs ', 2, None, '___sec62'),
              (' General first-order ODEs ', 1, None, '___sec63'),
              (' Generic form of first-order ODEs ', 2, None, '___sec64'),
              (' The $\\theta$-rule ', 2, None, '___sec65'),
              (' An implicit 2-step backward scheme ', 2, None, '___sec66'),
              (' Leapfrog schemes ', 2, None, '___sec67'),
              (' The ordinary Leapfrog scheme ', 3, None, '___sec68'),
              (' The filtered Leapfrog scheme ', 3, None, '___sec69'),
              (' The 2nd-order Runge-Kutta method ', 2, None, '___sec70'),
              (' A 2nd-order Taylor-series method ', 2, None, '___sec71'),
              (' The 2nd- and 3rd-order Adams-Bashforth schemes ',
               2,
               None,
               '___sec72'),
              (' The 4th-order Runge-Kutta method ',
               2,
               'decay:fd2:RK4',
               'decay:fd2:RK4'),
              (' The Odespy software ', 2, None, '___sec74'),
              (' Example: Runge-Kutta methods  ', 2, None, '___sec75'),
              (' Remark about using the $\\theta$-rule in Odespy ',
               3,
               None,
               '___sec76'),
              (' Example: Adaptive Runge-Kutta methods  ',
               2,
               'decay:fd2:adaptiveRK',
               'decay:fd2:adaptiveRK'),
              (' Exercises ', 1, None, '___sec78'),
              (' Exercise 8: Experiment with precision in tests and the size of $u$ ',
               2,
               'decay:fd2:exer:precision',
               'decay:fd2:exer:precision'),
              (' Exercise 9: Implement the 2-step backward scheme ',
               2,
               'decay:fd2:exer:bw2',
               'decay:fd2:exer:bw2'),
              (' Exercise 10: Implement the 2nd-order Adams-Bashforth scheme ',
               2,
               'decay:fd2:exer:AB2',
               'decay:fd2:exer:AB2'),
              (' Exercise 11: Implement the 3rd-order Adams-Bashforth scheme ',
               2,
               'decay:fd2:exer:AB3',
               'decay:fd2:exer:AB3'),
              (' Exercise 12: Analyze explicit 2nd-order methods ',
               2,
               'decay:exer:RK2:Taylor:analysis',
               'decay:exer:RK2:Taylor:analysis'),
              (' Problem 13: Implement and investigate the Leapfrog scheme ',
               2,
               'decay:fd2:exer:leapfrog1',
               'decay:fd2:exer:leapfrog1'),
              (' Problem 14: Make a unified implementation of many schemes ',
               2,
               'decay:fd2:exer:uni',
               'decay:fd2:exer:uni'),
              (' Applications of exponential decay models ',
               1,
               'decay:app',
               'decay:app'),
              (' Scaling ', 2, 'decay:app:scaling', 'decay:app:scaling'),
              (' Evolution of a population ',
               2,
               'decay:app:pop',
               'decay:app:pop'),
              (' Compound interest and inflation ',
               2,
               'decay:app:interest',
               'decay:app:interest'),
              (' Radioactive decay ',
               2,
               'decay:app:nuclear',
               'decay:app:nuclear'),
              (' Deterministic model ', 3, None, '___sec91'),
              (' Stochastic model ', 3, None, '___sec92'),
              (' Relation between stochastic and deterministic models ',
               3,
               None,
               '___sec93'),
              (" Newton's law of cooling ",
               2,
               'decay:app:Newton:cooling',
               'decay:app:Newton:cooling'),
              (' Decay of atmospheric pressure with altitude ',
               2,
               'decay:app:atm',
               'decay:app:atm'),
              (' Multiple atmospheric layers ', 3, None, '___sec96'),
              (' Simplification: $L=0$ ', 3, None, '___sec97'),
              (' Simplification: one-layer model ', 3, None, '___sec98'),
              (' Compaction of sediments ',
               2,
               'decay:app:sediment',
               'decay:app:sediment'),
              (' Vertical motion of a body in a viscous fluid ',
               2,
               'decay:app:drag',
               'decay:app:drag'),
              (' Overview of forces ', 3, None, '___sec101'),
              (' Equation of motion ', 3, None, '___sec102'),
              (' Terminal velocity ', 3, None, '___sec103'),
              (' A Crank-Nicolson scheme ', 3, None, '___sec104'),
              (' Physical data ', 3, None, '___sec105'),
              (' Verification ', 3, None, '___sec106'),
              (' Scaling ', 3, None, '___sec107'),
              (' Decay ODEs from solving a PDE by Fourier expansions ',
               2,
               'decay:app:diffusion:Fourier',
               'decay:app:diffusion:Fourier'),
              (' Exercises ', 1, None, '___sec109'),
              (" Exercise 15: Derive schemes for Newton's law of cooling ",
               2,
               'decay:app:exer:cooling:schemes',
               'decay:app:exer:cooling:schemes'),
              (" Exercise 16: Implement schemes for Newton's law of cooling ",
               2,
               'decay:app:exer:cooling:py',
               'decay:app:exer:cooling:py'),
              (' Exercise 17: Find time of murder from body temperature ',
               2,
               'decay:app:exer:cooling:murder',
               'decay:app:exer:cooling:murder'),
              (' Exercise 18: Simulate an oscillating cooling process ',
               2,
               'decay:app:exer:cooling:osc',
               'decay:app:exer:cooling:osc'),
              (' Exercise 19: Radioactive decay of Carbon-14 ',
               2,
               'decay:app:exer:radio:C14',
               'decay:app:exer:radio:C14'),
              (' Exercise 20: Simulate stochastic radioactive decay ',
               2,
               'decay:app:exer:stoch:nuclear',
               'decay:app:exer:stoch:nuclear'),
              (' Exercise 21: Radioactive decay of two substances ',
               2,
               'decay:app:exer:radio:twosubst',
               'decay:app:exer:radio:twosubst'),
              (' Exercise 22: Simulate the pressure drop in the atmosphere ',
               2,
               'decay:app:exer:atm1',
               'decay:app:exer:atm1'),
              (' Exercise 23: Make a program for vertical motion in a fluid ',
               2,
               'decay:app:exer:drag:prog',
               'decay:app:exer:drag:prog'),
              (' Project 24: Simulate parachuting ',
               2,
               'decay:app:exer:parachute',
               'decay:app:exer:parachute'),
              (' Exercise 25: Formulate vertical motion in the atmosphere ',
               2,
               'decay:app:exer:drag:atm1',
               'decay:app:exer:drag:atm1'),
              (' Exercise 26: Simulate vertical motion in the atmosphere ',
               2,
               'decay:app:exer:drag:atm2',
               'decay:app:exer:drag:atm2'),
              (' Exercise 27: Compute $y=|x|$ by solving an ODE ',
               2,
               'decay:app:exer:signum',
               'decay:app:exer:signum'),
              (' Exercise 28: Simulate growth of a fortune with random interest rate ',
               2,
               'decay:app:exer:interest',
               'decay:app:exer:interest'),
              (' Exercise 29: Simulate a population in a changing environment ',
               2,
               'decay:app:exer:pop:at',
               'decay:app:exer:pop:at'),
              (' Exercise 30: Simulate logistic growth ',
               2,
               'decay:app:exer:pop:logistic1',
               'decay:app:exer:pop:logistic1'),
              (' Exercise 31: Rederive the equation for continuous compound interest ',
               2,
               'decay:app:exer:interest:derive',
               'decay:app:exer:interest:derive'),
              (' Bibliography ', 1, None, '___sec127')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript"
 src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<!-- newcommands_keep.tex -->
$$
\newcommand{\uex}{{u_{\small\mbox{e}}}}
\newcommand{\Aex}{{A_{\small\mbox{e}}}}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\tp}{\thinspace .}
\newcommand{\E}[1]{\hbox{E}\lbrack #1 \rbrack}
\newcommand{\Var}[1]{\hbox{Var}\lbrack #1 \rbrack}
\newcommand{\Std}[1]{\hbox{Std}\lbrack #1 \rbrack}
\newcommand{\Oof}[1]{\mathcal{O}(#1)}
$$




    
<a name="part0005"></a>
<p>
<!-- begin top navigation -->
<table style="width: 100%"><tr><td>
<div style="text-align: left;"><a href="._decay-solarized004.html">&laquo; Previous</a></div>
</td><td>
<div style="text-align: right;"><a href="._decay-solarized006.html">Next &raquo;</a></div>
</td></tr></table>
<!-- end top navigation -->
</p>

<p>
<!-- !split -->

<h1 id="___sec49">Model extensions </h1>

<p>
It is time to consider generalizations of the simple decay model
\( u=-au \) and also to look at additional numerical solution methods.

<h2 id="___sec50">Generalization: including a variable coefficient </h2>

<p>
In the ODE for decay, \( u^{\prime}=-au \), we now consider the case where \( a \)
depends on time:

$$
\begin{equation}
u^{\prime}(t) = -a(t)u(t),\quad t\in (0,T],\quad u(0)=I \tp
\tag{44}
\end{equation}
$$

<p>
A Forward Euler scheme consist of evaluating <a href="#mjx-eqn-44">(44)</a>
at \( t=t_n \) and approximating the derivative with a forward
difference \( [D^+_t u]^n \):

$$
\begin{equation}
\frac{u^{n+1} - u^n}{\Delta t} = -a(t_n)u^n
\tp
\end{equation}
$$

The Backward Euler scheme becomes
$$
\begin{equation}
\frac{u^{n} - u^{n-1}}{\Delta t} = -a(t_n)u^n
\tp
\end{equation}
$$

The Crank-Nicolson method builds on sampling the ODE at
\( t_{n+\half} \). We can evaluate \( a \) at \( t_{n+\half} \)
and use an average for \( u \) at
times \( t_n \) and \( t_{n+1} \):
$$
\begin{equation}
\frac{u^{n+1} - u^{n}}{\Delta t} = -a(t_{n+\half})\half(u^n + u^{n+1})
\tp
\end{equation}
$$

Alternatively, we can use an average for the product \( au \):

$$
\begin{equation}
\frac{u^{n+1} - u^{n}}{\Delta t} = -\half(a(t_n)u^n + a(t_{n+1})u^{n+1})
\tp
\end{equation}
$$

The \( \theta \)-rule unifies the three mentioned schemes. One version is to
have \( a \) evaluated at \( t_{n+\theta} \),

$$
\begin{equation}
\frac{u^{n+1} - u^{n}}{\Delta t} = -a((1-\theta)t_n + \theta t_{n+1})((1-\theta) u^n + \theta u^{n+1})
\tp
\end{equation}
$$

Another possibility is to apply a weighted average for the product \( au \),
$$
\begin{equation}
\frac{u^{n+1} - u^{n}}{\Delta t} = -(1-\theta) a(t_n)u^n - \theta
a(t_{n+1})u^{n+1}
\tp
\end{equation}
$$

<p>
With the finite difference operator notation the Forward Euler and Backward
Euler schemes can be summarized as

$$
\begin{align}
\lbrack D^+_t u &= -au\rbrack^n,\\ 
\lbrack D^-_t u &= -au\rbrack^n
\tp
\end{align}
$$

The Crank-Nicolson and \( \theta \) schemes depend on whether we evaluate
\( a \) at the sample point for the ODE or if we use an average. The
various versions are written as

$$
\begin{align}
\lbrack D_t u &= -a\overline{u}^t\rbrack^{n+\half},\\ 
\lbrack D_t u &= -\overline{au}^t\rbrack^{n+\half},\\ 
\lbrack D_t u &= -a\overline{u}^{t,\theta}\rbrack^{n+\theta},\\ 
\lbrack D_t u &= -\overline{au}^{t,\theta}\rbrack^{n+\theta}
\tp
\end{align}
$$

<h2 id="decay:source">Generalization: including a source term</h2>

<p>
A further extension of the model ODE is to include a source term \( b(t) \):

$$
\begin{equation}
u^{\prime}(t) = -a(t)u(t) + b(t),\quad t\in (0,T],\quad u(0)=I
\tp
\tag{45}
\end{equation}
$$

<h3 id="___sec52">Schemes </h3>

<p>
The time point where we sample the ODE determines where \( b(t) \) is
evaluated. For the Crank-Nicolson scheme and the \( \theta \)-rule we
have a choice of whether to evaluate \( a(t) \) and \( b(t) \) at the
correct point or use an average. The chosen strategy becomes
particularly clear if we write up the schemes in the operator notation:

$$
\begin{align}
\lbrack D^+_t u &= -au + b\rbrack^n,\\ 
\lbrack D^-_t u &= -au + b\rbrack^n,\\ 
\lbrack D_t u   &= -a\overline{u}^t + b\rbrack^{n+\half},\\ 
\lbrack D_t u   &= \overline{-au+b}^t\rbrack^{n+\half},\\ 
\lbrack D_t u   &= -a\overline{u}^{t,\theta} + b\rbrack^{n+\theta},\\ 
\lbrack D_t u   &= \overline{-au+b}^{t,\theta}\rbrack^{n+\theta}
\tag{46}
\tp
\end{align}
$$

<h2 id="decay:general">Implementation of the generalized model problem</h2>

<h3 id="___sec54">Deriving the \( \theta \)-rule formula </h3>

<p>
Writing out the \( \theta \)-rule in <a href="#mjx-eqn-46">(46)</a>,
using <a href="._decay-solarized001.html#mjx-eqn-28">(28)</a>
and <a href="._decay-solarized001.html#mjx-eqn-29">(29)</a>, we get
$$
\begin{equation}
\frac{u^{n+1}-u^n}{\Delta t} = \theta(-a^{n+1}u^{n+1} + b^{n+1}))
+ (1-\theta)(-a^nu^{n} + b^n)),
\tag{47}
\end{equation}
$$

where \( a^n \) means evaluating \( a \) at \( t=t_n \) and similar for
\( a^{n+1} \), \( b^n \), and \( b^{n+1} \).
We solve for \( u^{n+1} \):
$$
\begin{equation}
u^{n+1} = ((1 - \Delta t(1-\theta)a^n)u^n
+ \Delta t(\theta b^{n+1} + (1-\theta)b^n))(1 + \Delta t\theta a^{n+1})^{-1}
\tp
\end{equation}
$$

<h3 id="___sec55">The Python code </h3>

<p>
Here is a suitable implementation of <a href="#mjx-eqn-47">(47)</a>
where \( a(t) \) and \( b(t) \) are given as
Python functions:

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">solver</span>(I, a, b, T, dt, theta):
    <span style="color: #CD5555">&quot;&quot;&quot;</span>
<span style="color: #CD5555">    Solve u&#39;=-a(t)*u + b(t), u(0)=I,</span>
<span style="color: #CD5555">    for t in (0,T] with steps of dt.</span>
<span style="color: #CD5555">    a and b are Python functions of t.</span>
<span style="color: #CD5555">    &quot;&quot;&quot;</span>
    dt = <span style="color: #658b00">float</span>(dt)            <span style="color: #228B22"># avoid integer division</span>
    Nt = <span style="color: #658b00">int</span>(<span style="color: #658b00">round</span>(T/dt))     <span style="color: #228B22"># no of time intervals</span>
    T = Nt*dt                 <span style="color: #228B22"># adjust T to fit time step dt</span>
    u = zeros(Nt+<span style="color: #B452CD">1</span>)           <span style="color: #228B22"># array of u[n] values</span>
    t = linspace(<span style="color: #B452CD">0</span>, T, Nt+<span style="color: #B452CD">1</span>)  <span style="color: #228B22"># time mesh</span>

    u[<span style="color: #B452CD">0</span>] = I                  <span style="color: #228B22"># assign initial condition</span>
    <span style="color: #8B008B; font-weight: bold">for</span> n <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">0</span>, Nt):    <span style="color: #228B22"># n=0,1,...,Nt-1</span>
        u[n+<span style="color: #B452CD">1</span>] = ((<span style="color: #B452CD">1</span> - dt*(<span style="color: #B452CD">1</span>-theta)*a(t[n]))*u[n] + \ 
                  dt*(theta*b(t[n+<span style="color: #B452CD">1</span>]) + (<span style="color: #B452CD">1</span>-theta)*b(t[n])))/\ 
                  (<span style="color: #B452CD">1</span> + dt*theta*a(t[n+<span style="color: #B452CD">1</span>]))
    <span style="color: #8B008B; font-weight: bold">return</span> u, t
</pre></div>
<p>
This function is found in the file <a href="http://tinyurl.com/nm5587k/decay/decay_vc.py" target="_self"><tt>decay_vc.py</tt></a> (<code>vc</code> stands for "variable coefficients").

<h3 id="___sec56">Coding of variable coefficients </h3>

<p>
The <code>solver</code> function shown above demands the arguments <code>a</code> and <code>b</code> to
be Python functions of time <code>t</code>, say

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">a</span>(t):
    <span style="color: #8B008B; font-weight: bold">return</span> a_0 <span style="color: #8B008B; font-weight: bold">if</span> t &lt; tp <span style="color: #8B008B; font-weight: bold">else</span> k*a_0

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">b</span>(t):
    <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">1</span>
</pre></div>
<p>
Here, <code>a(t)</code> has three parameters <code>a0</code>, <code>tp</code>, and <code>k</code>,
which must be global variables.
A better implementation is to represent <code>a</code> by a class where the
parameters are attributes and a <em>special method</em> <code>__call__</code> evaluates \( a(t) \):

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span style="color: #8B008B; font-weight: bold">class</span> <span style="color: #008b45; font-weight: bold">A</span>:
    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">__init__</span>(<span style="color: #658b00">self</span>, a0=<span style="color: #B452CD">1</span>, k=<span style="color: #B452CD">2</span>):
        <span style="color: #658b00">self</span>.a0, <span style="color: #658b00">self</span>.k = a0, k

    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">__call__</span>(<span style="color: #658b00">self</span>, t):
        <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #658b00">self</span>.a0 <span style="color: #8B008B; font-weight: bold">if</span> t &lt; <span style="color: #658b00">self</span>.tp <span style="color: #8B008B; font-weight: bold">else</span> <span style="color: #658b00">self</span>.k*<span style="color: #658b00">self</span>.a0

a = A(a0=<span style="color: #B452CD">2</span>, k=<span style="color: #B452CD">1</span>)  <span style="color: #228B22"># a behaves as a function a(t)</span>
</pre></div>
<p>
For quick tests it is cumbersome to write a complete function or a class.
The <em>lambda function</em> construction in Python is then convenient. For example,
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%">a = <span style="color: #8B008B; font-weight: bold">lambda</span> t: a_0 <span style="color: #8B008B; font-weight: bold">if</span> t &lt; tp <span style="color: #8B008B; font-weight: bold">else</span> k*a_0
</pre></div>
<p>
is equivalent to the <code>def a(t):</code> definition above. In general,
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%">f = <span style="color: #8B008B; font-weight: bold">lambda</span> arg1, arg2, ...: expressin
</pre></div>
<p>
is equivalent to
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">f</span>(arg1, arg2, ...):
    <span style="color: #8B008B; font-weight: bold">return</span> expression
</pre></div>
<p>
One can use lambda functions directly in calls. Say we want to
solve \( u^{\prime}=-u+1 \), \( u(0)=2 \):
<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%">u, t = solver(<span style="color: #B452CD">2</span>, <span style="color: #8B008B; font-weight: bold">lambda</span> t: <span style="color: #B452CD">1</span>, <span style="color: #8B008B; font-weight: bold">lambda</span> t: <span style="color: #B452CD">1</span>, T, dt, theta)
</pre></div>
<p>
A lambda function can appear anywhere where a variable can appear.

<h2 id="decay:verify:trivial">Verifying a constant solution</h2>

<p>
A very useful partial verification method is to construct a test
problem with a very simple solution, usually \( u=\hbox{const} \).
Especially the initial debugging of a program code can benefit greatly
from such tests, because 1) all relevant numerical methods will
exactly reproduce a constant solution, 2) many of the intermediate
calculations are easy to control for a constant \( u \), and 3) even a
constant \( u \) can uncover many bugs in an implementation.

<p>
The only constant solution for the problem \( u^{\prime}=-au \) is \( u=0 \), but too
many bugs can escape from that trivial solution.  It is much better to
search for a problem where \( u=C=\hbox{const}\neq 0 \).  Then \( u^{\prime}=-a(t)u
+ b(t) \) is more appropriate: with \( u=C \) we can choose any \( a(t) \) and
set \( b=a(t)C \) and \( I=C \). An appropriate test function is

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">test_constant_solution</span>():
    <span style="color: #CD5555">&quot;&quot;&quot;</span>
<span style="color: #CD5555">    Test problem where u=u_const is the exact solution, to be</span>
<span style="color: #CD5555">    reproduced (to machine precision) by any relevant method.</span>
<span style="color: #CD5555">    &quot;&quot;&quot;</span>
    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">exact_solution</span>(t):
        <span style="color: #8B008B; font-weight: bold">return</span> u_const

    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">a</span>(t):
        <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">2.5</span>*(<span style="color: #B452CD">1</span>+t**<span style="color: #B452CD">3</span>)  <span style="color: #228B22"># can be arbitrary</span>

    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">b</span>(t):
        <span style="color: #8B008B; font-weight: bold">return</span> a(t)*u_const

    u_const = <span style="color: #B452CD">2.15</span>
    theta = <span style="color: #B452CD">0.4</span>; I = u_const; dt = <span style="color: #B452CD">4</span>
    Nt = <span style="color: #B452CD">4</span>  <span style="color: #228B22"># enough with a few steps</span>
    u, t = solver(I=I, a=a, b=b, T=Nt*dt, dt=dt, theta=theta)
    <span style="color: #8B008B; font-weight: bold">print</span> u
    u_e = exact_solution(t)
    difference = <span style="color: #658b00">abs</span>(u_e - u).max()  <span style="color: #228B22"># max deviation</span>
    tol = <span style="color: #B452CD">1E-14</span>
    <span style="color: #8B008B; font-weight: bold">assert</span> difference &lt; tol
</pre></div>
<p>
An interesting question is what type of bugs that will make the
computed \( u^n \) deviate from the exact solution \( C \).
Fortunately, the updating formula and the initial condition must
be absolutely correct for the test to pass! Any attempt to make
a wrong indexing in terms like <code>a(t[n])</code> or any attempt to
introduce an erroneous factor in the formula creates a solution
that is different from \( C \).

<h2 id="decay:MMS">Verification via manufactured solutions</h2>

<p>
Following the idea of the previous section, we can choose any formula
as the exact solution, insert the formula in the ODE problem and fit
the data \( a(t) \), \( b(t) \), and \( I \) to make the chosen
formula fulfill the equation. This
powerful technique for generating exact solutions is very useful for
verification purposes and known as the <em>method of manufactured
solutions</em>, often abbreviated MMS.

<p>
One common choice of solution is a linear function in the independent
variable(s). The rationale behind such a simple variation is that
almost any relevant numerical solution method for differential
equation problems is able to reproduce the linear function exactly to
machine precision (if \( u \) is about unity in size; precision is lost if
\( u \) take on large values, see <a href="#decay:fd2:exer:precision">Exercise 8: Experiment with precision in tests and the size of \( u \)</a>).
The linear solution also makes some stronger demands to the
numerical method and the implementation than the constant solution
used in the section <a href="#decay:verify:trivial">Verifying a constant solution</a>, at least in more
complicated applications. However, the constant solution is often
ideal for initial debugging before proceeding with a linear solution.

<p>
We choose a linear solution \( u(t) = ct + d \). From the initial condition it
follows that \( d=I \).
Inserting this \( u \) in the ODE results in
$$ c = -a(t)u + b(t) \tp  $$

Any function \( u=ct+I \) is then a correct solution if we choose
$$ b(t) = c + a(t)(ct + I) \tp  $$

With this \( b(t) \) there are no restrictions on \( a(t) \) and \( c \).

<p>
Let prove that such a linear solution obeys the numerical
schemes. To this end, we must check that \( u^n = ca(t_n)(ct_n+I) \)
fulfills the discrete equations. For these calculations, and
later calculations involving linear solutions inserted in
finite difference schemes, it is convenient to
compute the action of a difference operator on a linear function \( t \):

$$
\begin{align}
\lbrack D_t^+ t\rbrack^n &= \frac{t_{n+1}-t_n}{\Delta t}=1,
\tag{48}\\ 
\lbrack D_t^- t\rbrack^n &= \frac{t_{n}-t_{n-1}}{\Delta t}=1,
\tag{49}\\ 
\lbrack D_t t\rbrack^n &= \frac{t_{n+\half}-t_{n-\half}}{\Delta t}=\frac{(n+\half)\Delta t - (n-\half)\Delta t}{\Delta t}=1
\tag{50}
\tp
\end{align}
$$

Clearly, all three finite difference approximations to the derivative are
exact for \( u(t)=t \) or its mesh function counterpart \( u^n = t_n \).

<p>
The difference equation for the Forward Euler scheme

$$ [D^+_t u = -au + b]^n, $$

with \( a^n=a(t_n) \), \( b^n=c + a(t_n)(ct_n + I) \), and \( u^n=ct_n + I \)
then results in

$$ c = -a(t_n)(ct_n+I) + c + a(t_n)(ct_n + I) = c $$

which is always fulfilled. Similar calculations can be done for the
Backward Euler and Crank-Nicolson schemes, or the \( \theta \)-rule for
that matter. In all cases, \( u^n=ct_n +I \) is an exact solution of
the discrete equations. That is why we should expect that
\( u^n - \uex(t_n) =0 \) mathematically and \( |u^n - \uex(t_n)| \) less
than a small number about the machine precision for \( n=0,\ldots,N_t \).

<p>
The following function offers an implementation of this verification
test based on a linear exact solution:

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">test_linear_solution</span>():
    <span style="color: #CD5555">&quot;&quot;&quot;</span>
<span style="color: #CD5555">    Test problem where u=c*t+I is the exact solution, to be</span>
<span style="color: #CD5555">    reproduced (to machine precision) by any relevant method.</span>
<span style="color: #CD5555">    &quot;&quot;&quot;</span>
    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">exact_solution</span>(t):
        <span style="color: #8B008B; font-weight: bold">return</span> c*t + I

    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">a</span>(t):
        <span style="color: #8B008B; font-weight: bold">return</span> t**<span style="color: #B452CD">0.5</span>  <span style="color: #228B22"># can be arbitrary</span>

    <span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">b</span>(t):
        <span style="color: #8B008B; font-weight: bold">return</span> c + a(t)*exact_solution(t)

    theta = <span style="color: #B452CD">0.4</span>; I = <span style="color: #B452CD">0.1</span>; dt = <span style="color: #B452CD">0.1</span>; c = -<span style="color: #B452CD">0.5</span>
    T = <span style="color: #B452CD">4</span>
    Nt = <span style="color: #658b00">int</span>(T/dt)  <span style="color: #228B22"># no of steps</span>
    u, t = solver(I=I, a=a, b=b, T=Nt*dt, dt=dt, theta=theta)
    u_e = exact_solution(t)
    difference = <span style="color: #658b00">abs</span>(u_e - u).max()  <span style="color: #228B22"># max deviation</span>
    <span style="color: #8B008B; font-weight: bold">print</span> difference
    tol = <span style="color: #B452CD">1E-14</span>  <span style="color: #228B22"># depends on c!</span>
    <span style="color: #8B008B; font-weight: bold">assert</span> difference &lt; tol
</pre></div>
<p>
Any error in the updating formula makes this test fail!

<p>
Choosing more complicated formulas as the exact solution, say
\( \cos(t) \), will not make the numerical and exact solution
coincide to machine precision, because finite differencing of
\( \cos(t) \) does not exactly yield the exact derivative \( -\sin(t) \).
In such cases, the verification procedure
must be based on measuring the convergence rates as exemplified in
the section <a href="#decay:convergence:rate">Computing convergence rates</a>. Convergence rates can be
computed as long as one has
an exact solution of a problem that the solver can be tested on, but
this can always be obtained by the method of manufactured solutions.

<h2 id="decay:convergence:rate">Computing convergence rates</h2>

<p>
We expect that the error \( E \) in the numerical solution is
reduced if the mesh size \( \Delta t \) is decreased. More specifically,
many numerical methods obey a power-law relation between \( E \) and
\( \Delta t \):

$$
\begin{equation}
E = C\Delta t^r,
\tag{51}
\end{equation}
$$

where \( C \) and \( r \) are (usually unknown) constants independent of \( \Delta t \).
The formula <a href="#mjx-eqn-51">(51)</a> is viewed as an asymptotic model valid for
sufficiently small \( \Delta t \). How small is normally hard to estimate
without doing numerical estimations of \( r \).

<p>
The parameter \( r \) is known as the <em>convergence rate</em>. For example,
if the convergence rate is 2, halving \( \Delta t \) reduces the error by
a factor of 4. Diminishing \( \Delta t \) then has a greater impact on
the error compared with methods that have \( r=1 \). For a given value of \( r \),
we refer to the method as of \( r \)-th order. First- and second-order
methods are most common in scientific computing.

<h3 id="___sec60">Estimating \( r \) </h3>

<p>
There are two alternative ways of estimating \( C \) and \( r \) based on a set of
\( m \) simulations with corresponding pairs \( (\Delta t_i, E_i) \), \( i=0,\ldots,m-1 \),
and \( \Delta t_{i} < \Delta t_{i-1} \) (i.e., decreasing cell size).

<ol>
 <li> Take the logarithm of <a href="#mjx-eqn-51">(51)</a>, \( \ln E = r\ln \Delta t + \ln C \),
    and fit a straight line to the data points \( (\Delta t_i, E_i) \),
    \( i=0,\ldots,m-1 \).</li>
 <li> Consider two consecutive experiments, \( (\Delta t_i, E_i) \) and
    \( (\Delta t_{i-1}, E_{i-1}) \). Dividing the equation
    \( E_{i-1}=C\Delta t_{i-1}^r \) by \( E_{i}=C\Delta t_{i}^r \) and solving
    for \( r \) yields</li>
</ol>

$$
\begin{equation}
r_{i-1} = \frac{\ln (E_{i-1}/E_i)}{\ln (\Delta t_{i-1}/\Delta t_i)}
\tag{52}
\end{equation}
$$

for \( i=1,\ldots,m-1 \).

<p>
The disadvantage of method 1 is that <a href="#mjx-eqn-51">(51)</a> might not be valid
for the coarsest meshes (largest \( \Delta t \) values). Fitting a line
to all the data points is then misleading.  Method 2 computes
convergence rates for pairs of experiments and allows us to see
if the sequence \( r_i \) converges to some value as \( i\rightarrow m-2 \).
The final \( r_{m-2} \) can then be taken as the convergence rate.
If the coarsest meshes have a differing rate, the corresponding
time steps are probably too large for <a href="#mjx-eqn-51">(51)</a> to be valid.
That is, those time steps lie outside the asymptotic range of
\( \Delta t \) values where the error behaves like <a href="#mjx-eqn-51">(51)</a>.

<h3 id="___sec61">Implementation </h3>

<p>
Suppose we have some function <code>error(dt, ...)</code> that can compute the numerical
error in our mathematical model. To compute
\( r_0, r_1, \ldots, r_{m-2} \)
from <a href="#mjx-eqn-51">(51)</a> we just wrap a loop over \( \Delta t \) values
(<code>dt_values</code>) around
the <code>error</code> function:

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%">E_values = [error(dt, ...) <span style="color: #8B008B; font-weight: bold">for</span> dt <span style="color: #8B008B">in</span> dt_values]

<span style="color: #228B22"># Compute pairwise convergence rates</span>
m = <span style="color: #658b00">len</span>(dt_values)
r = [log(E_values[i-<span style="color: #B452CD">1</span>]/E_values[i])/
     log(dt_values[i-<span style="color: #B452CD">1</span>]/dt_values[i])
     <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">1</span>, m, <span style="color: #B452CD">1</span>)]

<span style="color: #228B22"># Strip off to 2 decimals</span>
r = [<span style="color: #658b00">round</span>(r_, <span style="color: #B452CD">2</span>) <span style="color: #8B008B; font-weight: bold">for</span> r_ <span style="color: #8B008B">in</span> r]
</pre></div>
<p>
We can run the convergence rate estimate computations for the
\( \theta \)-rule discretization of \( u'=-au \), using \( \Delta t = 0.5,-.25, 0.1, 0.05, 0.025, 0.01 \):

<p>

<!-- code=text typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%">FE: 1.33 1.15 1.07 1.03 1.02
BE: 0.98 0.99 0.99 1.00 1.00
CN: 2.14 2.07 2.03 2.01 2.01
</pre></div>
<p>
The Forward and Backward Euler methods seem to have an \( r \) value which
stabilizes at 1, while the Crank-Nicolson seems to be a second-order
method with \( r=2 \). These results are in very good agreement with
various theoretical considerations for \( r \).

<p>
<div class="alert alert-block alert-notice alert-text-normal">
<b>Why convergence rates are important.</b>
<p>
The strong practical application of computing convergence rates is for
verification: wrong convergence rates point to errors in the code, and
correct convergence rates brings evidence that the implementation is
correct. Experience shows that bugs in the code easily destroy the
expected convergence rate.
</div>


<h2 id="___sec62">Extension to systems of ODEs </h2>

<p>
Many ODE models involves more than one unknown function and more
than one equation. Here is an example of two unknown functions \( u(t) \)
and \( v(t) \):

$$
\begin{align}
u^{\prime} &= a u + bv,\\ 
v^{\prime} &= cu +  dv,
\end{align}
$$

for constants \( a,b,c,d \).
Applying the Forward Euler method to each equation results in simple
updating formula

$$
\begin{align}
u^{n+1} &= u^n + \Delta t (a u^n + b v^n),\\ 
v^{n+1} &= u^n + \Delta t (cu^n + dv^n)
\tp
\end{align}
$$

On the other hand, the Crank-Nicolson or Backward Euler schemes result in a
\( 2\times 2 \) linear system for the new unknowns. The latter schemes gives

$$
\begin{align}
u^{n+1} &= u^n + \Delta t (a u^{n+1} + b v^{n+1}),\\ 
v^{n+1} &= v^n + \Delta t (c u^{n+1} + d v^{n+1})\tp
\end{align}
$$

Collecting \( u^{n+1} \) as well as \( v^{n+1} \) on the left-hand side results
in
$$
\begin{align}
(1 - \Delta t a)u^{n+1} + bv^{n+1} &= u^n ,\\ 
c u^{n+1} + (1 - \Delta t d) v^{n+1} &= v^n ,
\end{align}
$$

which is a system of two coupled, linear, algebraic equations in two
unknowns.

<h1 id="___sec63">General first-order ODEs </h1>

<p>
We now turn the attention to general, nonlinear ODEs and systems of
such ODEs.  Our focus is on numerical methods that can be readily
reused for time-discretization PDEs, and diffusion PDEs in particular.
The methods are just briefly listed, and we refer to the rich literature
for more detailed descriptions and analysis - the books
<a href="._decay-solarized007.html#Petzold_Ascher_1998">[2]</a> <a href="._decay-solarized007.html#Griffiths_et_al_2010">[3]</a> <a href="._decay-solarized007.html#Hairer_Wanner_Norsett_bookI">[4]</a> <a href="._decay-solarized007.html#Hairer_Wanner_bookII">[5]</a> are all excellent resources on numerical methods for ODEs.
We also demonstrate the Odespy Python interface to a range
of different software for general first-order ODE systems.

<h2 id="___sec64">Generic form of first-order ODEs </h2>

<p>
ODEs are commonly written in the generic form

$$
\begin{equation}
u^{\prime} = f(u,t),\quad u(0)=I,
\tag{53}
\end{equation}
$$

where \( f(u,t) \)  is some prescribed function.
As an example, our most
general exponential decay model <a href="#mjx-eqn-45">(45)</a> has
\( f(u,t)=-a(t)u(t) + b(t) \).

<p>
The unknown \( u \) in <a href="#mjx-eqn-53">(53)</a> may either be
a scalar function of time \( t \), or a vector valued function of \( t \) in
case of a <em>system of ODEs</em> with \( m \) unknown components:
$$ u(t) = (u^{(0)}(t),u^{(1)}(t),\ldots,u^{(m-1)}(t)) \tp  $$

In that case, the right-hand side is vector-valued function with \( m \)
components,
$$
\begin{align*}
f(u, t) = ( & f^{(0)}(u^{(0)}(t),\ldots,u^{(m-1)}(t)),\\ 
            & f^{(1)}(u^{(0)}(t),\ldots,u^{(m-1)}(t)),\\ 
            & \vdots,\\ 
            & f^{(m-1)}(u^{(0)}(t),\ldots,u^{(m-1)}(t)))
\tp
\end{align*}
$$

<p>
Actually, any system of ODEs can
be written in the form <a href="#mjx-eqn-53">(53)</a>, but higher-order
ODEs then need auxiliary unknown functions to enable conversion to
a first-order system.

<p>
Next we list some well-known methods for \( u^{\prime}=f(u,t) \), valid both for
a single ODE (scalar \( u \)) and systems of ODEs (vector \( u \)).
The choice of methods is inspired by the kind of schemes that are
popular also for time discretization of partial differential equations.

<h2 id="___sec65">The \( \theta \)-rule </h2>

<p>
The \( \theta \)-rule scheme applied to \( u^{\prime}=f(u,t) \) becomes

$$
\begin{equation}
\frac{u^{n+1}-u^n}{\Delta t} = \theta f(u^{n+1},t_{n+1}) +
(1-\theta)f(u^n, t_n)\tp
\tag{54}
\end{equation}
$$

Bringing the unknown \( u^{n+1} \) to the left-hand side and the known terms
on the right-hand side gives

$$
\begin{equation}
u^{n+1} - \Delta t \theta f(u^{n+1},t_{n+1}) =
u^n + \Delta t(1-\theta)f(u^n, t_n)\tp
\end{equation}
$$

For a general \( f \) (not linear in \( u \)), this equation is <em>nonlinear</em> in
the unknown \( u^{n+1} \) unless \( \theta = 0 \). For a scalar ODE (\( m=1 \)),
we have to solve a single nonlinear algebraic equation for \( u^{n+1} \),
while for a system of ODEs, we get a system of coupled, nonlinear
algebraic equations. Newton's method is a popular solution approach
in both cases. Note that with the Forward Euler scheme (\( \theta =0 \))
we do not have to deal with nonlinear equations, because in that
case we have an explicit updating formula for \( u^{n+1} \). This is known
as an <em>explicit</em> scheme. With \( \theta\neq 1 \) we have to solve
(systems of) algebraic equations, and the scheme is said to be <em>implicit</em>.

<h2 id="___sec66">An implicit 2-step backward scheme </h2>

<p>
The implicit backward method with 2 steps applies a
three-level backward difference as approximation to \( u^{\prime}(t) \),
$$ u^{\prime}(t_{n+1}) \approx \frac{3u^{n+1} - 4u^{n} + u^{n-1}}{2\Delta t},$$

which is an approximation of order \( \Delta t^2 \) to the first derivative.
The resulting scheme for \( u^{\prime}=f(u,t) \) reads
$$
\begin{equation}
u^{n+1} = \frac{4}{3}u^n - \frac{1}{3}u^{n-1} +
\frac{2}{3}\Delta t f(u^{n+1}, t_{n+1})
\tp
\tag{55}
\end{equation}
$$

Higher-order versions of the scheme <a href="#mjx-eqn-55">(55)</a> can
be constructed by including more time levels. These schemes are known
as the Backward Differentiation Formulas (BDF), and the particular
version <a href="#mjx-eqn-55">(55)</a> is often referred to as BDF2.

<p>
Note that the scheme <a href="#mjx-eqn-55">(55)</a> is implicit and requires
solution of nonlinear equations when \( f \) is nonlinear in \( u \).  The
standard 1st-order Backward Euler method or the Crank-Nicolson scheme
can be used for the first step.

<h2 id="___sec67">Leapfrog schemes </h2>

<h3 id="___sec68">The ordinary Leapfrog scheme </h3>

<p>
The derivative of \( u \) at some point \( t_n \) can be approximated by
a central difference over two time steps,

$$
\begin{equation}
u^{\prime}(t_n)\approx \frac{u^{n+1}-u^{n-1}}{2\Delta t} = [D_{2t}u]^n
\end{equation}
$$

which is an approximation of second order in \( \Delta t \). The scheme
can then be written as

$$ [D_{2t}u=f(u,t)]^n, $$

in operator notation. Solving for \( u^{n+1} \) gives

$$
\begin{equation}
u^{n+1} = u^{n-1} + \Delta t f(u^n, t_n)
\tp
\tag{56}
\end{equation}
$$

Observe that <a href="#mjx-eqn-56">(56)</a> is an explicit scheme, and that
a nonlinear \( f \) (in \( u \)) is trivial to handle since it only involves
the known \( u^n \) value.
Some other scheme must be used as starter to compute \( u^1 \), preferably
the Forward Euler scheme since it is also explicit.

<h3 id="___sec69">The filtered Leapfrog scheme </h3>

<p>
Unfortunately, the Leapfrog scheme <a href="#mjx-eqn-56">(56)</a>
will develop growing oscillations with time (see <a href="#decay:fd2:exer:leapfrog1">Problem 13: Implement and investigate the Leapfrog scheme</a>)[[[. A remedy for such undesired oscillations
is to introduce a <em>filtering technique</em>. First, a standard Leapfrog
step is taken, according to <a href="#mjx-eqn-56">(56)</a>, and then
the previous \( u^n \) value is adjusted according to
$$
\begin{equation}
u^n\ \leftarrow\ u^n + \gamma (u^{n-1} - 2u^n + u^{n+1})
\tag{57}
\tp
\end{equation}
$$

The \( \gamma \)-terms will effectively damp oscillations in the solution,
especially those with short wavelength (like point-to-point oscillations).
A common choice of \( \gamma \) is 0.6 (a value used in the
famous NCAR Climate Model).

<p>
<!-- Need to elaborate more on this: -->
<!-- The difference in th \( \gamma \) term in <a href="#mjx-eqn-57">(57)</a> -->
<!-- can be recognized as a finite difference approximation to -->
<!-- \( \Delta t^2 u^{\prime\prime}(t_n) \). -->

<h2 id="___sec70">The 2nd-order Runge-Kutta method </h2>

<p>
The two-step scheme

$$
\begin{align}
u^* &= u^n + \Delta t f(u^n, t_n),
\tag{58}\\ 
u^{n+1} &= u^n + \Delta t \half \left( f(u^n, t_n) + f(u^*, t_{n+1})
\right),
\tag{59}
\end{align}
$$

essentially applies a Crank-Nicolson method <a href="#mjx-eqn-59">(59)</a>
to the ODE, but replaces
the term \( f(u^{n+1}, t_{n+1}) \) by a prediction
\( f(u^{*}, t_{n+1}) \) based on a Forward Euler step <a href="#mjx-eqn-58">(58)</a>.
The scheme <a href="#mjx-eqn-58">(58)</a>-<a href="#mjx-eqn-59">(59)</a> is
known as Huen's method, but is also a 2nd-order Runge-Kutta method.
The scheme is explicit, and the error is expected to behave as \( \Delta t^2 \).

<h2 id="___sec71">A 2nd-order Taylor-series method </h2>

<p>
One way to compute \( u^{n+1} \) given \( u^n \) is to use a Taylor polynomial.
We may write up a polynomial of 2nd degree:
$$
u^{n+1} = u^n + u^{\prime}(t_n)\Delta t + {\half}u^{\prime\prime}(t_n)\Delta t^2
\tp
$$

From the equation \( u^{\prime}=f(u,t) \) it follows that the derivatives of \( u \)
can be expressed in terms of \( f \) and its derivatives:
$$
\begin{align*}
u^{\prime}(t_n) &=f(u^n,t_n),\\ 
u^{\prime\prime}(t_n) &=
\frac{\partial f}{\partial u}(u^n,t_n) u^{\prime}(t_n) + \frac{\partial f}{\partial t}\\ 
&=  f(u^n,t_n)\frac{\partial f}{\partial u}(u^n,t_n)  +
\frac{\partial f}{\partial t},
\end{align*}
$$

resulting in the scheme
$$
\begin{equation}
u^{n+1} = u^n + f(u^n,t_n)\Delta t + \half\left(
f(u^n,t_n)\frac{\partial f}{\partial u}(u^n,t_n)  +
\frac{\partial f}{\partial t}\right)\Delta t^2
\tp
\tag{60}
\end{equation}
$$

More terms in the series could be included in the Taylor polynomial to
obtain methods of higher order than 2.

<h2 id="___sec72">The 2nd- and 3rd-order Adams-Bashforth schemes </h2>

<p>
The following method is known as the 2nd-order Adams-Bashforth scheme:

$$
\begin{equation}
u^{n+1} = u^n + \half\Delta t\left( 3f(u^n, t_n) - f(u^{n-1}, t_{n-1})
\right)
\tp
\tag{61}
\end{equation}
$$

The scheme is explicit and requires another one-step scheme to compute
\( u^1 \) (the Forward Euler scheme or Heun's method, for instance).
As the name implies, the scheme is of order \( \Delta t^2 \).

<p>
Another explicit scheme, involving four time levels, is the
3rd-order Adams-Bashforth scheme

$$
\begin{equation}
u^{n+1} = u^n + \frac{1}{12}\left( 23f(u^n, t_n) - 16 f(u^{n-1},t_{n-1})
+ 5f(u^{n-2}, t_{n-2})\right)
\tp
\tag{62}
\end{equation}
$$

The numerical error is of order \( \Delta t^3 \), and the scheme needs
some method for computing \( u^1 \) and \( u^2 \).

<p>
More general, higher-order Adams-Bashforth schemes (also called
<em>explicit Adams methods</em>) compute \( u^{n+1} \) as a linear combination
of \( f \) at \( k \) previous time steps:

$$ u^{n+1} = u^n + \sum_{j=0}^k \beta_jf(u^{n-j},t_{n-j}),$$

where \( \beta_j \) are known coefficients.

<h2 id="decay:fd2:RK4">The 4th-order Runge-Kutta method</h2>

<p>
The perhaps most widely used method to solve ODEs is the 4th-order
Runge-Kutta method, often called RK4.
Its derivation is a nice illustration of common
numerical approximation strategies, so let us go through the
steps in detail.

<p>
The starting point is to integrate the ODE
\( u^{\prime}=f(u,t) \) from \( t_n \) to \( t_{n+1} \):

$$ u(t_{n+1}) - u(t_n) = \int\limits_{t_{n}}^{t_{n+1}} f(u(t),t)dt\tp $$

We want to compute \( u(t_{n+1}) \) and regard \( u(t_n) \) as known.
The task is to find good approximations for the integral, since the
integrand involves the unknown \( u \) between \( t_n \) and \( t_{n+1} \).

<p>
The integral can be approximated by the famous
<a href="http://en.wikipedia.org/wiki/Simpson's_rule" target="_self">Simpson's rule</a>:

$$ \int\limits_{t_{n}}^{t_{n+1}} f(u(t),t)dt
\approx \frac{\Delta t}{6}\left( f^n + 4f^{n+\half} + f^{n+1}\right)\tp$$

The problem now is that we do not know \( f^{n+\half}=f(u^{n+\half},t_{n+\half}) \)
and \( f^{n+1}=(u^{n+1},t_{n+1}) \) as we know only \( u^n \) and hence \( f^n \).
The idea is to use various approximations for \( f^{n+\half} \) and
\( f^{n+1} \) based on using well-known schemes for the ODE in the
intervals \( [t_n,t_{n+\half}] \) and \( [t_n, t_{n+1}] \).
We split the integral approximation into four terms:

$$ \int\limits_{t_{n}}^{t_{n+1}} f(u(t),t)dt
\approx \frac{\Delta t}{6}\left( f^n + 2\hat{f}^{n+\half}
+ 2\tilde{f}^{n+\half} + \bar{f}^{n+1}\right),$$

where \( \hat{f}^{n+\half} \), \( \tilde{f}^{n+\half} \), and \( \bar{f}^{n+1} \)
are approximations to \( f^{n+\half} \) and
\( f^{n+1} \) that can be based on already computed quantities.
For \( \hat{f}^{n+\half} \) we can apply
an approximation to \( u^{n+\half} \) using the Forward Euler
method with step \( \half\Delta t \):

$$
\begin{equation}
\hat{f}^{n+\half} = f(u^n + \half{\Delta t} f^n, t_{n+\half})
\tag{63}
\end{equation}
$$

Since this gives us a prediction of \( f^{n+\half} \), we can for
\( \tilde{f}^{n+\half} \) try a Backward Euler method to approximate \( u^{n+\half} \):

$$
\begin{equation}
\tilde{f}^{n+\half} = f(u^n + \half\Delta t\hat{f}^{n+\half}, t_{n+\half})\tp
\tag{64}
\end{equation}
$$

With \( \tilde{f}^{n+\half} \) as a hopefully good approximation to
\( f^{n+\half} \), we can for the final term \( \bar{f}^{n+1} \) use
a Crank-Nicolson method to approximate \( u^{n+1} \):

$$
\begin{equation}
\bar{f}^{n+1} = f(u^n + \Delta t \hat{f}^{n+\half}, t_{n+1})\tp
\tag{65}
\end{equation}
$$

We have now used the Forward and Backward Euler methods as well as the
Crank-Nicolson method in the context of Simpson's rule. The hope is
that the combination of these methods yields an overall time-stepping
scheme from \( t_n \) to \( t_n{+1} \) that is much more accurate than the
\( \Oof{\Delta t} \) and \( \Oof{\Delta t^2} \) of the individual steps.
This is indeed true: the overall accuracy is \( \Oof{\Delta t^4} \)!

<p>
To summarize, the 4th-order Runge-Kutta method becomes

$$
\begin{equation}
u^{n+1} = u^n +
\frac{\Delta t}{6}\left( f^n + 2\hat{f}^{n+\half}
+ 2\tilde{f}^{n+\half} + \bar{f}^{n+1}\right),
\end{equation}
$$

where the quantities on the right-hand side are computed from
<a href="#mjx-eqn-63">(63)</a>-<a href="#mjx-eqn-65">(65)</a>. Note that
the scheme is fully explicit so there is never any need to solve linear or
nonlinear algebraic
equations. However, the stability is conditional and depends on \( f \).
There is a whole range of <em>implicit</em> Runge-Kutta methods that
are unconditionally stable, but require solution of algebraic
equations involving \( f \) at each time step.

<p>
The simplest way to explore more sophisticated methods for ODEs is to
apply one of the many high-quality software packages that exist, as the
next section explains.

<h2 id="___sec74">The Odespy software </h2>

<p>
A wide range of the methods and software exist for solving <a href="#mjx-eqn-53">(53)</a>.
Many of methods are accessible through a unified Python interface offered
by the <a href="https://github.com/hplgit/odespy" target="_self">Odespy</a> <a href="._decay-solarized007.html#odespy">[6]</a> package.
Odespy features simple Python implementations of the most fundamental
schemes as well as Python interfaces to several famous packages for
solving ODEs: <a href="https://computation.llnl.gov/casc/odepack/odepack_home.html" target="_self">ODEPACK</a>, <a href="https://computation.llnl.gov/casc/odepack/odepack_home.html" target="_self">Vode</a>,
<a href="http://www.netlib.org/ode/rkc.f" target="_self">rkc.f</a>, <a href="http://www.netlib.org/ode/rkf45.f" target="_self">rkf45.f</a>, <a href="http://www.unige.ch/~hairer/software.html" target="_self">Radau5</a>, as well
as the ODE solvers in <a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.ode.html" target="_self">SciPy</a>, <a href="http://docs.sympy.org/dev/modules/mpmath/calculus/odes.html" target="_self">SymPy</a>, and <a href="http://olivierverdier.github.com/odelab/" target="_self">odelab</a>.

<p>
The usage of Odespy follows this setup for the ODE \( u^{\prime}=-au \),
\( u(0)=I \), \( t\in (0,T] \), here solved
by the famous 4th-order Runge-Kutta method, using \( \Delta t=1 \)
and \( N_t=6 \) steps:

<p>

<!-- code=text typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%">def f(u, t):
    return -a*u

import odespy
import numpy as np

I = 1; a = 0.5; Nt = 6; dt = 1
solver = odespy.RK4(f)
solver.set_initial_condition(I)
t_mesh = np.linspace(0, Nt*dt, Nt+1)
u, t = solver.solve(t_mesh)
</pre></div>
<p>
The previously listed methods for ODEs are all accessible in
Odespy:

<ul>
 <li> the \( \theta \)-rule: <code>ThetaRule</code></li>
 <li> special cases of the \( \theta \)-rule: <code>ForwardEuler</code>, <code>BackwardEuler</code>,
   <code>CrankNicolson</code></li>
 <li> the 2nd- and 4th-order Runge-Kutta methods: <code>RK2</code> and <code>RK4</code></li>
 <li> The BDF methods and the Adam-Bashforth methods:
   <code>Vode</code>, <code>Lsode</code>, <code>Lsoda</code>, <code>lsoda_scipy</code></li>
 <li> The Leapfrog scheme: <code>Leapfrog</code> and <code>LeapfrogFiltered</code></li>
</ul>

<h2 id="___sec75">Example: Runge-Kutta methods  </h2>

<p>
Since all solvers have the same interface in Odespy, modulo different set of
parameters to the solvers' constructors, one can easily make a list of
solver objects and run a loop for comparing (a lot of) solvers. The
code below, found in complete form in <a href="http://tinyurl.com/nm5587k/decay/decay_odespy.py" target="_self"><tt>decay_odespy.py</tt></a>,
compares the famous Runge-Kutta methods of orders 2, 3, and 4
with the exact solution of the decay equation
\( u^{\prime}=-au \).
Since we have quite long time steps, we have included the only
relevant \( \theta \)-rule for large time steps, the Backward Euler scheme
(\( \theta=1 \)), as well.
Figure <a href="#decay:odespy:fig1">16</a> shows the results.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">scitools.std</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">sys</span>

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">f</span>(u, t):
    <span style="color: #8B008B; font-weight: bold">return</span> -a*u

I = <span style="color: #B452CD">1</span>; a = <span style="color: #B452CD">2</span>; T = <span style="color: #B452CD">6</span>
dt = <span style="color: #658b00">float</span>(sys.argv[<span style="color: #B452CD">1</span>]) <span style="color: #8B008B; font-weight: bold">if</span> <span style="color: #658b00">len</span>(sys.argv) &gt;= <span style="color: #B452CD">2</span> <span style="color: #8B008B; font-weight: bold">else</span> <span style="color: #B452CD">0.75</span>
Nt = <span style="color: #658b00">int</span>(<span style="color: #658b00">round</span>(T/dt))
t = np.linspace(<span style="color: #B452CD">0</span>, Nt*dt, Nt+<span style="color: #B452CD">1</span>)

solvers = [odespy.RK2(f),
           odespy.RK3(f),
           odespy.RK4(f),]

<span style="color: #228B22"># BackwardEuler must use Newton solver to converge</span>
<span style="color: #228B22"># (Picard is default and leads to divergence)</span>
solvers.append(
    odespy.BackwardEuler(f, nonlinear_solver=<span style="color: #CD5555">&#39;Newton&#39;</span>))
<span style="color: #228B22"># Or tell BackwardEuler that it is a linear problem</span>
solvers[-<span style="color: #B452CD">1</span>] = odespy.BackwardEuler(f, f_is_linear=<span style="color: #658b00">True</span>,
                                   jac=<span style="color: #8B008B; font-weight: bold">lambda</span> u, t: -a)]
legends = []
<span style="color: #8B008B; font-weight: bold">for</span> solver <span style="color: #8B008B">in</span> solvers:
    solver.set_initial_condition(I)
    u, t = solver.solve(t)

    plt.plot(t, u)
    plt.hold(<span style="color: #CD5555">&#39;on&#39;</span>)
    legends.append(solver.__class__.__name__)

<span style="color: #228B22"># Compare with exact solution plotted on a very fine mesh</span>
t_fine = np.linspace(<span style="color: #B452CD">0</span>, T, <span style="color: #B452CD">10001</span>)
u_e = I*np.exp(-a*t_fine)
plt.plot(t_fine, u_e, <span style="color: #CD5555">&#39;-&#39;</span>) <span style="color: #228B22"># avoid markers by specifying line type</span>
legends.append(<span style="color: #CD5555">&#39;exact&#39;</span>)

plt.legend(legends)
plt.title(<span style="color: #CD5555">&#39;Time step: %g&#39;</span> % dt)
plt.show()
</pre></div>
<p>
With the <code>odespy.BackwardEuler</code> method we either have to tell that
the problem is linear and provide the Jacobian of \( f(u,t) \), i.e.,
\( \partial f/\partial u \), as the <code>jac</code> argument, or we have to assume
that \( f \) is nonlinear, but then specify Newton's method as solver
for the nonlinear equations (since the equations are linear, Newton's
method will converge in one iteration). By default,
<code>odespy.BackwardEuler</code> assumes a nonlinear problem to be solved by
Picard iteration, but that leads to divergence in the present problem.

<p>
<div class="alert alert-block alert-notice alert-text-normal">
<b>Visualization tip.</b>
<p>
We use Matplotlib for
plotting here, but importing <code>scitools.std</code> as <code>plt</code> instead
also works. Plain use of Matplotlib as done here results in
curves with different colors, which may be hard to distinguish on
black-and-white paper. Using <code>scitools.std</code>, curves are
automatically given colors <em>and</em> markers, thus making curves easy
to distinguish on screen with colors and on black-and-white paper.
The automatic adding of markers is normally a bad idea for a
very fine mesh since all the markers get cluttered, but <code>scitools.std</code> limits
the number of markers in such cases.
For the exact solution we use a very fine mesh, but in the code
above we specify the line type as a solid line (<code>-</code>), which means
no markers and just a color to be automatically determined by
the backend used for plotting (Matplotlib by default, but
<code>scitools.std</code> gives the opportunity to use other backends
to produce the plot, e.g., Gnuplot or Grace).

<p>
Also note the that the legends
are based on the class names of the solvers, and in Python the name of
a the class type (as a string) of an object <code>obj</code> is obtained by
<code>obj.__class__.__name__</code>.
</div>


<p>
<center> <!-- figure -->
<hr class="figure">
<center><p class="caption">Figure 16:  Behavior of different schemes for the decay equation. <div id="decay:odespy:fig1"></div> </p></center>
<p><img src="fig-decay/decay_odespy1_png.png" align="bottom" width=600></p>
</center>

<p>
The runs in Figure <a href="#decay:odespy:fig1">16</a>
and other experiments reveal that the 2nd-order Runge-Kutta
method (<code>RK2</code>) is unstable for \( \Delta t>1 \) and decays slower than the
Backward Euler scheme for large and moderate \( \Delta t \) (see <a href="#decay:exer:RK2:Taylor:analysis">Exercise 12: Analyze explicit 2nd-order methods</a> for an analysis).  However, for
fine \( \Delta t = 0.25 \) the 2nd-order Runge-Kutta method approaches
the exact solution faster than the Backward Euler scheme.  That is,
the latter scheme does a better job for larger \( \Delta t \), while the
higher order scheme is superior for smaller \( \Delta t \). This is a
typical trend also for most schemes for ordinary and partial
differential equations.

<p>
The 3rd-order Runge-Kutta method (<code>RK3</code>) has also artifacts in form
of oscillatory behavior for the larger \( \Delta t \) values, much
like that of the Crank-Nicolson scheme. For finer \( \Delta t \),
the 3rd-order Runge-Kutta method converges quickly to the exact
solution.

<p>
The 4th-order Runge-Kutta method (<code>RK4</code>) is slightly inferior
to the Backward Euler scheme on the coarsest mesh, but is then
clearly superior to all the other schemes. It is definitely the
method of choice for all the tested schemes.

<h3 id="___sec76">Remark about using the \( \theta \)-rule in Odespy </h3>

<p>
The Odespy package assumes that the ODE is written as \( u^{\prime}=f(u,t) \) with
an \( f \) that is possibly nonlinear in \( u \). The \( \theta \)-rule for
\( u^{\prime}=f(u,t) \) leads to
$$ u^{n+1} = u^{n} + \Delta t\left(\theta f(u^{n+1}, t_{n+1})
+ (1-\theta) f(u^{n}, t_{n})\right),$$

which is a <em>nonlinear equation</em> in \( u^{n+1} \). Odespy's implementation
of the \( \theta \)-rule (<code>ThetaRule</code>) and the specialized Backward Euler
(<code>BackwardEuler</code>) and Crank-Nicolson (<code>CrankNicolson</code>) schemes
must invoke iterative methods for
solving the nonlinear equation in \( u^{n+1} \). This is done even when
\( f \) is linear in \( u \), as in the model problem \( u^{\prime}=-au \), where we can
easily solve for \( u^{n+1} \) by hand.  Therefore, we need to specify
use of Newton's method to solve the equations.
(Odespy allows other methods than Newton's to be used, for instance
Picard iteration, but that method is not suitable. The reason is that it
applies the Forward Euler scheme to generate a start value for
the iterations. Forward Euler may give very wrong solutions
for large \( \Delta t \) values. Newton's method, on the other hand,
is insensitive to the start value in <em>linear problems</em>.)

<h2 id="decay:fd2:adaptiveRK">Example: Adaptive Runge-Kutta methods</h2>

<p>
Odespy offers solution methods that can adapt the size of \( \Delta t \)
with time to match a desired accuracy in the solution. Intuitively,
small time steps will be chosen in areas where the solution is changing
rapidly, while larger time steps can be used where the solution
is slowly varying. Some kind of <em>error estimator</em> is used to
adjust the next time step at each time level.

<p>
A very popular adaptive method for solving ODEs is the Dormand-Prince
Runge-Kutta method of order 4 and 5. The 5th-order method is used as a
reference solution and the difference between the 4th- and 5th-order
methods is used as an indicator of the error in the numerical
solution.  The Dormand-Prince method is the default choice in MATLAB's
widely used <code>ode45</code> routine.

<p>
We can easily set up Odespy to use the Dormand-Prince method and
see how it selects the optimal time steps. To this end, we request
only one time step from \( t=0 \) to \( t=T \) and ask the method to
compute the necessary non-uniform time mesh to meet a certain
error tolerance. The code goes like

<p>

<!-- code=python (!bc pypro) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">odespy</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">decay_mod</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">sys</span>
<span style="color: #228B22">#import matplotlib.pyplot as plt</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">scitools.std</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">plt</span>

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">f</span>(u, t):
    <span style="color: #8B008B; font-weight: bold">return</span> -a*u

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">exact_solution</span>(t):
    <span style="color: #8B008B; font-weight: bold">return</span> I*np.exp(-a*t)

I = <span style="color: #B452CD">1</span>; a = <span style="color: #B452CD">2</span>; T = <span style="color: #B452CD">5</span>
tol = <span style="color: #658b00">float</span>(sys.argv[<span style="color: #B452CD">1</span>])
solver = odespy.DormandPrince(f, atol=tol, rtol=<span style="color: #B452CD">0.1</span>*tol)

Nt = <span style="color: #B452CD">1</span>  <span style="color: #228B22"># just one step - let the scheme find its intermediate points</span>
t_mesh = np.linspace(<span style="color: #B452CD">0</span>, T, Nt+<span style="color: #B452CD">1</span>)
t_fine = np.linspace(<span style="color: #B452CD">0</span>, T, <span style="color: #B452CD">10001</span>)

solver.set_initial_condition(I)
u, t = solver.solve(t_mesh)

<span style="color: #228B22"># u and t will only consist of [I, u^Nt] and [0,T]</span>
<span style="color: #228B22"># solver.u_all and solver.t_all contains all computed points</span>
plt.plot(solver.t_all, solver.u_all, <span style="color: #CD5555">&#39;ko&#39;</span>)
plt.hold(<span style="color: #CD5555">&#39;on&#39;</span>)
plt.plot(t_fine, exact_solution(t_fine), <span style="color: #CD5555">&#39;b-&#39;</span>)
plt.legend([<span style="color: #CD5555">&#39;tol=%.0E&#39;</span> % tol, <span style="color: #CD5555">&#39;exact&#39;</span>])
plt.savefig(<span style="color: #CD5555">&#39;tmp_odespy_adaptive.png&#39;</span>)
plt.show()
</pre></div>
<p>
Running four cases with tolerances \( 10^{-1} \), \( 10^{-3} \), \( 10^{-5} \),
and \( 10^{-7} \), gives the results in Figure <a href="#decay:odespy:fig2">17</a>.
Intuitively, one would expect denser points in the beginning of
the decay and larger time steps when the solution flattens out.

<p>
<center> <!-- figure -->
<hr class="figure">
<center><p class="caption">Figure 17:  Choice of adaptive time mesh by the Dormand-Prince method for different tolerances. <div id="decay:odespy:fig2"></div> </p></center>
<p><img src="fig-decay/decay_DormandPrince_adaptivity.png" align="bottom" width=800,></p>
</center>

<h1 id="___sec78">Exercises </h1>

<p>
<!-- --- begin exercise --- -->

<h2 id="decay:fd2:exer:precision">Exercise 8: Experiment with precision in tests and the size of \( u \)</h2>

<p>
It is claimed in the section <a href="#decay:MMS">Verification via manufactured solutions</a> that most numerical methods will
reproduce a linear exact solution to machine precision. Test this
assertion using the test function <code>test_linear_solution</code> in the
<a href="http://tinyurl.com/nm5587k/decay/decay_vc.py" target="_self"><tt>decay_vc.py</tt></a> program.
Vary the parameter <code>c</code> from very small, via <code>c=1</code> to many larger values,
and print out the maximum difference between the numerical solution
and the exact solution. What is the relevant value of the tolerance
in the float comparison in each case?
Filename: <code>test_precision.py</code>.

<p>
<!-- --- end exercise --- -->

<p>
<!-- --- begin exercise --- -->

<h2 id="decay:fd2:exer:bw2">Exercise 9: Implement the 2-step backward scheme</h2>

<p>
Implement the 2-step backward method <a href="#mjx-eqn-55">(55)</a> for the
model \( u^{\prime}(t) = -a(t)u(t) + b(t) \), \( u(0)=I \).  Allow the first step to
be computed by either the Backward Euler scheme or the Crank-Nicolson
scheme. Verify the implementation by choosing \( a(t) \) and \( b(t) \) such
that the exact solution is linear in \( t \) (see the section <a href="#decay:MMS">Verification via manufactured solutions</a>). Show mathematically that a linear solution is indeed a
solution of the discrete equations.

<p>
Compute convergence rates (see the section <a href="#decay:convergence:rate">Computing convergence rates</a>) in
a test case \( a=\hbox{const} \) and \( b=0 \), where we easily have an exact
solution, and determine if the choice of a first-order scheme
(Backward Euler) for the first step has any impact on the overall
accuracy of this scheme. The expected error goes like \( \Oof{\Delta t^2} \).
Filename: <code>decay_backward2step.py</code>.

<p>
<!-- --- end exercise --- -->

<p>
<!-- --- begin exercise --- -->

<h2 id="decay:fd2:exer:AB2">Exercise 10: Implement the 2nd-order Adams-Bashforth scheme</h2>

<p>
Implement the 2nd-order Adams-Bashforth method <a href="#mjx-eqn-61">(61)</a>
for the decay problem \( u^{\prime}=-a(t)u + b(t) \), \( u(0)=I \), \( t\in (0, T] \).
Use the Forward Euler method for the first step such that the overall
scheme is explicit. Verify the implementation using an exact
solution that is linear in time.
Analyze the scheme by searching for solutions \( u^n=A^n \) when \( a=\hbox{const} \)
and \( b=0 \). Compare this second-order secheme to the Crank-Nicolson scheme.
Filename: <code>decay_AdamsBashforth2.py</code>.

<p>
<!-- --- end exercise --- -->

<p>
<!-- --- begin exercise --- -->

<h2 id="decay:fd2:exer:AB3">Exercise 11: Implement the 3rd-order Adams-Bashforth scheme</h2>

<p>
Implement the 3rd-order Adams-Bashforth method <a href="#mjx-eqn-62">(62)</a>
for the decay problem \( u^{\prime}=-a(t)u + b(t) \), \( u(0)=I \), \( t\in (0, T] \).
Since the scheme is explicit, allow it to be started by two steps with
the Forward Euler method.  Investigate experimentally the case where
\( b=0 \) and \( a \) is a constant: Can we have oscillatory solutions for
large \( \Delta t \)?
Filename: <code>decay_AdamsBashforth3.py</code>.

<p>
<!-- --- end exercise --- -->

<p>
<!-- --- begin exercise --- -->

<h2 id="decay:exer:RK2:Taylor:analysis">Exercise 12: Analyze explicit 2nd-order methods</h2>

<p>
Show that the schemes <a href="#mjx-eqn-59">(59)</a> and
<a href="#mjx-eqn-60">(60)</a> are identical in the case \( f(u,t)=-a \), where
\( a>0 \) is a constant. Assume that the numerical solution reads
\( u^n=A^n \) for some unknown amplification factor \( A \) to be determined.
Find \( A \) and derive stability criteria. Can the scheme produce
oscillatory solutions of \( u^{\prime}=-au \)? Plot the numerical and exact
amplification factor.
Filename: <code>decay_RK2_Taylor2.py</code>.

<p>
<!-- --- end exercise --- -->

<p>
<!-- --- begin exercise --- -->

<h2 id="decay:fd2:exer:leapfrog1">Problem 13: Implement and investigate the Leapfrog scheme</h2>

<p>
A Leapfrog scheme
for the ODE \( u^{\prime}(t) = -a(t)u(t) + b(t) \) is defined by

$$ \lbrack D_{2t}u = -au+b\rbrack^n\tp$$

A separate method is needed to compute \( u^1 \). The Forward Euler
scheme is a possible candidate.

<p>
<b>a)</b>
Implement the Leapfrog scheme for the model equation.
Plot the solution in the case \( a=1 \), \( b=0 \), \( I=1 \),
\( \Delta t = 0.01 \), \( t\in [0,4] \). Compare with the exact
solution \( \uex(t)=e^{-t} \).

<p>
<b>b)</b>
Show mathematically that a linear solution in \( t \) fulfills the
Forward Euler scheme for the first step and the Leapfrog scheme
for the subsequent steps. Use this linear solution to verify
the implementation, and automate the verification through a test
function.

<p>
<!-- --- begin hint in exercise --- -->

<p>
<b>Hint.</b>
It can be wise to automate the calculations such that it is easy to
redo the calculations for other types of solutions. Here is
a possible <code>sympy</code> function that takes a symbolic expression <code>u</code>
(implemented as a Python function of <code>t</code>), fits the <code>b</code> term, and
checks if <code>u</code> fulfills the discrete equations:

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">sympy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">sp</span>

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">analyze</span>(u):
    t, dt, a = sp.symbols(<span style="color: #CD5555">&#39;t dt a&#39;</span>)

    <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&#39;Analyzing u_e(t)=%s&#39;</span> % u(t)
    <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&#39;u(0)=%s&#39;</span> % u(t).subs(t, <span style="color: #B452CD">0</span>)

    <span style="color: #228B22"># Fit source term to the given u(t)</span>
    b = sp.diff(u(t), t) + a*u(t)
    b = sp.simplify(b)
    <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&#39;Source term b:&#39;</span>, b

    <span style="color: #228B22"># Residual in discrete equations; Forward Euler step</span>
    R_step1 = (u(t+dt) - u(t))/dt + a*u(t) - b
    R_step1 = sp.simplify(R_step1)
    <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&#39;Residual Forward Euler step:&#39;</span>, R_step1

    <span style="color: #228B22"># Residual in discrete equations; Leapfrog steps</span>
    R = (u(t+dt) - u(t-dt))/(<span style="color: #B452CD">2</span>*dt) + a*u(t) - b
    R = sp.simplify(R)
    <span style="color: #8B008B; font-weight: bold">print</span> <span style="color: #CD5555">&#39;Residual Leapfrog steps:&#39;</span>, R

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">u_e</span>(t):
    <span style="color: #8B008B; font-weight: bold">return</span> c*t + I

analyze(u_e)
<span style="color: #228B22"># or short form: analyze(lambda t: c*t + I)</span>
</pre></div>
<p>
<!-- --- end hint in exercise --- -->

<p>
<b>c)</b>
Show that a second-order polynomial in \( t \) cannot be a solution of the discrete
equations. However, if a Crank-Nicolson scheme is used for the first
step, a second-order polynomial solves the equations exactly.

<p>
<b>d)</b>
Create a manufactured solution \( u(t)=\sin(t) \) for the ODE
\( u^{\prime}=-au+b \).
Compute the convergence rate of the Leapfrog scheme using this
manufactured solution. The expected convergence rate of the
Leapfrog scheme is \( \Oof{\Delta t^2} \). Does the use of a
1st-order method for the first step impact the convergence rate?

<p>
<!-- A possible test case is -->
<!-- \( u^{\prime}=-au + b \), \( u(0)=0 \), where \( \uex(t)=b/a + (I - b/a)e^{-at} \) if -->
<!-- \( a \) and \( b \) are constants. -->

<p>
<b>e)</b>
Set up a set of experiments to demonstrate that the Leapfrog scheme
<a href="#mjx-eqn-56">(56)</a> is associated with numerical artifacts
(instabilities). Document the main results from this investigation.

<p>
<b>f)</b>
Analyze and explain the
instabilities of the Leapfrog scheme <a href="#mjx-eqn-56">(56)</a>:

<ol>
<li> Choose \( a=\mbox{const} \) and \( b=0 \). Assume that an exact solution
   of the discrete equations has
   the form \( u^n=A^n \), where \( A \) is an amplification factor to
   be determined. Derive an equation for \( A \) by inserting \( u^n=A^n \)
   in the Leapfrog scheme.</li>
<li> Compute \( A \) either by hand and/or with the aid of <code>sympy</code>.
   The polynomial for \( A \) has two roots, \( A_1 \) and \( A_2 \). Let
   \( u^n \) be a linear combination \( u^n=C_1A_1^n + C_2A_2^n \).</li>
<li> Show that one of the roots is the explanation of the instability.</li>
<li> Compare \( A \) with the exact expression, using a Taylor series approximation.</li>
<li> How can \( C_1 \) and \( C_2 \) be determined?</li>
</ol>

<b>g)</b>
Since the original Leapfrog scheme is unconditionally unstable as time
grows, it demands some stabilization.  This can be done by filtering,
where we first find \( u^{n+1} \) from the original Leapfrog scheme and
then replace \( u^{n} \) by \( u^n + \gamma (u^{n-1} - 2u^n +
u^{n+1}) \), where \( \gamma \) can be taken as 0.6.  Implement the filtered
Leapfrog scheme and check that it can handle tests where the original
Leapfrog scheme is unstable.

<p>
Filenames: <code>decay_leapfrog.py</code>, <code>decay_leapfrog.pdf</code>.

<p>
<!-- --- end exercise --- -->

<p>
<!-- --- begin exercise --- -->

<h2 id="decay:fd2:exer:uni">Problem 14: Make a unified implementation of many schemes</h2>

<p>
Consider the linear ODE problem \( u^{\prime}(t)=-a(t)u(t) + b(t) \), \( u(0)=I \).
Explicit schemes for this problem can be written in the general form
$$
\begin{equation}
u^{n+1} = \sum_{j=0}^m c_ju^{n-j},
\tag{66}
\end{equation}
$$

for some choice of \( c_0,\ldots,c_m \).
Find expressions for the \( c_j \) coefficients in case of the
\( \theta \)-rule, the three-level backward scheme,
the Leapfrog scheme, the 2nd-order Runge-Kutta method,
and the 3rd-order Adams-Bashforth scheme.

<p>
Make a class <code>ExpDecay</code> that implements the
general updating formula <a href="#mjx-eqn-66">(66)</a>.
The formula cannot be applied for \( n < m \), and for those \( n \) values, other
schemes must be used. Assume for simplicity that we just
repeat Crank-Nicolson steps until <a href="#mjx-eqn-66">(66)</a> can be used.
Use a subclass
to specify the list \( c_0,\ldots,c_m \) for a particular method, and
implement subclasses for all the mentioned schemes.
Verify the implementation by testing with a linear solution, which should
be exactly reproduced by all methods.
Filename: <code>decay_schemes_oo.py</code>.

<p>
<!-- --- end exercise --- -->

<p>
<p>
<!-- begin bottom navigation -->
<table style="width: 100%"><tr><td>
<div style="text-align: left;"><a href="._decay-solarized004.html">&laquo; Previous</a></div>
</td><td>
<div style="text-align: right;"><a href="._decay-solarized006.html">Next &raquo;</a></div>
</td></tr></table>
<!-- end bottom navigation -->
</p>

<!-- ------------------- end of main content --------------- -->


</body>
</html>
    

