<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="description" content="Introduction to computing with finite difference methods">
<meta name="keywords" content="decay ODE,exponential decay,mesh,grid,mesh function,finite differences,forward difference,finite differences forward,difference equation,discrete equation,algebraic equation,finite difference scheme,Forward Euler scheme,backward difference,finite differences backward,backward scheme, 1-step,Backward Euler scheme,Crank-Nicolson scheme,centered difference,finite differences centered,averaging arithmetic,weighted average,theta-rule,$\theta$-rule,finite difference operator notation,operator notation, finite differences,directory,folder,doc strings,printf format,format string syntax (Python),plotting curves,visualizing curves,representative (mesh function),array arithmetics,array computing,continuous function norms,norm continuous,discrete function norms,mesh function norms,norm discrete (mesh function),error norms,scalar computing,plotting curves,visualizing curves,PNG plot,PDF plot,EPS plot,viewing graphics files,cropping images,stability,amplification factor,A-stable methods,L-stable methods,error amplification factor,error global,consistency,stability,convergence,lambda functions,method of manufactured solutions,MMS (method of manufactured solutions),implicit schemes,explicit schemes,theta-rule,$\theta$-rule,backward scheme, 2-step,BDF2 scheme,Leapfrog scheme,Leapfrog scheme, filtered,Heun's method,Runge-Kutta, 2nd-order method,Taylor-series methods (for ODEs),Adams-Bashforth scheme, 2nd-order,Adams-Bashforth scheme, 3rd order,Runge-Kutta, 4th-order method,RK4,adaptive time stepping,Dormand-Prince Runge-Kutta 4-5 method,population dynamics,logistic model,radioactive decay,terminal velocity,geometric mean,averaging geometric,scaling">

<title>Introduction to computing with finite difference methods</title>


<link href="https://raw.githubusercontent.com/hplgit/doconce/master/bundled/html_styles/style_solarized_box/css/solarized_light_code.css" rel="stylesheet" type="text/css" title="light"/>
<script src="http://www.peterhaschke.com/assets/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<link href="http://thomasf.github.io/solarized-css/solarized-light.min.css" rel="stylesheet">
<style type="text/css">
h1 {color: #b58900;}  /* yellow */
/* h1 {color: #cb4b16;}  orange */
/* h1 {color: #d33682;}  magenta, the original choice of thomasf */
.alert-text-small   { font-size: 80%;  }
.alert-text-large   { font-size: 130%; }
.alert-text-normal  { font-size: 90%;  }
.alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:1px solid #93a1a1;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  color: #555;
  background-color: #eee8d5;
  background-position: 10px 5px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 55px;
  width: 75%;
 }
.alert-block {padding-top:14px; padding-bottom:14px}
.alert-block > p, .alert-block > ul {margin-bottom:1em}
.alert li {margin-top: 1em}
.alert-block p+p {margin-top:5px}
.alert-notice { background-image: url(https://raw.github.com/hplgit/doconce/master/bundled/html_images/small_yellow_notice.png); }
.alert-summary  { background-image:url(https://raw.github.com/hplgit/doconce/master/bundled/html_images/small_yellow_summary.png); }
.alert-warning { background-image: url(https://raw.github.com/hplgit/doconce/master/bundled/html_images/small_yellow_warning.png); }
.alert-question {background-image:url(https://raw.github.com/hplgit/doconce/master/bundled/html_images/small_yellow_question.png); }

div { text-align: justify; text-justify: inter-word; }
</style>

</head>

<!-- tocinfo
{'highest level': 1,
 'sections': [(' Finite difference methods  ',
               1,
               'decay:basics',
               'decay:basics'),
              (' A basic model for exponential decay ',
               2,
               'decay:model',
               'decay:model'),
              (' The Forward Euler scheme ',
               2,
               'decay:schemes:FE',
               'decay:schemes:FE'),
              (' Step 1: Discretizing the domain ', 3, None, '___sec3'),
              (' Step 2: Fulfilling the equation at discrete time points ',
               3,
               None,
               '___sec4'),
              (' Step 3: Replacing derivatives by finite differences ',
               3,
               None,
               '___sec5'),
              (' Step 4: Formulating a recursive algorithm ',
               3,
               None,
               '___sec6'),
              (' The Backward Euler scheme ',
               2,
               'decay:schemes:BE',
               'decay:schemes:BE'),
              (' The Crank-Nicolson scheme ',
               2,
               'decay:schemes:CN',
               'decay:schemes:CN'),
              (' The unifying $\\theta$-rule ',
               2,
               'decay:schemes:theta',
               'decay:schemes:theta'),
              (' Constant time step ', 2, None, '___sec10'),
              (' Compact operator notation for finite differences ',
               2,
               'decay:fd:op',
               'decay:fd:op'),
              (' Implementation ', 1, 'decay:impl1', 'decay:impl1'),
              (' Making a solver function ', 2, 'decay:py1', 'decay:py1'),
              (' Function for computing the numerical solution ',
               3,
               None,
               '___sec14'),
              (' Integer division ', 3, None, '___sec15'),
              (' Doc strings ', 3, None, '___sec16'),
              (' Formatting of numbers ', 3, None, '___sec17'),
              (' Running the program ', 3, None, '___sec18'),
              (' Plotting the solution ', 3, None, '___sec19'),
              (' Verifying the implementation ', 2, None, '___sec20'),
              (' Running a few algorithmic steps by hand ',
               3,
               None,
               '___sec21'),
              (' Computing the numerical error as a mesh function ',
               2,
               'decay:computing:error',
               'decay:computing:error'),
              (' Computing the norm of the numerical error ',
               2,
               'decay:computing:error:norm',
               'decay:computing:error:norm'),
              (' Scalar computing ', 3, None, '___sec24'),
              (' Plotting solutions ', 2, 'decay:plotting', 'decay:plotting'),
              (' Experiments with computing and plotting ',
               2,
               None,
               '___sec26'),
              (' Combining plot files ', 3, None, '___sec27'),
              (' Plotting with SciTools ', 3, None, '___sec28'),
              (' Memory-saving implementation ', 2, None, '___sec29'),
              (' Analysis of finite difference equations ',
               1,
               'decay:analysis',
               'decay:analysis'),
              (' Experimental investigation of oscillatory solutions ',
               2,
               None,
               '___sec31'),
              (' Exact numerical solution ', 2, None, '___sec32'),
              (' Stability ', 2, None, '___sec33'),
              (' Comparing amplification factors ', 2, None, '___sec34'),
              (' Series expansion of amplification factors ',
               2,
               None,
               '___sec35'),
              (' The fraction of numerical and exact amplification factors ',
               2,
               None,
               '___sec36'),
              (' The global error at a point ',
               2,
               'decay:analysis:gobal:error',
               'decay:analysis:gobal:error'),
              (' Integrated errors ', 2, None, '___sec38'),
              (' Truncation error ', 2, None, '___sec39'),
              (' Consistency, stability, and convergence ',
               2,
               None,
               '___sec40'),
              (' Exercises ', 1, None, '___sec41'),
              (' Exercise 1: Visualize the accuracy of finite differences ',
               2,
               'decay:analysis:exer:fd:exp:plot',
               'decay:analysis:exer:fd:exp:plot'),
              (' Exercise 2: Explore the $\\theta$-rule for exponential growth ',
               2,
               'decay:analysis:exer:growth',
               'decay:analysis:exer:growth'),
              (' Model extensions ', 1, None, '___sec44'),
              (' Generalization: including a variable coefficient ',
               2,
               None,
               '___sec45'),
              (' Generalization: including a source term ',
               2,
               'decay:source',
               'decay:source'),
              (' Schemes ', 3, None, '___sec47'),
              (' Implementation of the generalized model problem ',
               2,
               'decay:general',
               'decay:general'),
              (' Deriving the $\\theta$-rule formula ', 3, None, '___sec49'),
              (' The Python code ', 3, None, '___sec50'),
              (' Coding of variable coefficients ', 3, None, '___sec51'),
              (' Verifying a constant solution ',
               2,
               'decay:verify:trivial',
               'decay:verify:trivial'),
              (' Verification via manufactured solutions ',
               2,
               'decay:MMS',
               'decay:MMS'),
              (' Extension to systems of ODEs ', 2, None, '___sec54'),
              (' General first-order ODEs ', 1, None, '___sec55'),
              (' Generic form of first-order ODEs ', 2, None, '___sec56'),
              (' The $\\theta$-rule ', 2, None, '___sec57'),
              (' An implicit 2-step backward scheme ', 2, None, '___sec58'),
              (' Leapfrog schemes ', 2, None, '___sec59'),
              (' The ordinary Leapfrog scheme ', 3, None, '___sec60'),
              (' The filtered Leapfrog scheme ', 3, None, '___sec61'),
              (' The 2nd-order Runge-Kutta method ', 2, None, '___sec62'),
              (' A 2nd-order Taylor-series method ', 2, None, '___sec63'),
              (' The 2nd- and 3rd-order Adams-Bashforth schemes ',
               2,
               None,
               '___sec64'),
              (' The 4th-order Runge-Kutta method ',
               2,
               'decay:fd2:RK4',
               'decay:fd2:RK4'),
              (' The Odespy software ', 2, None, '___sec66'),
              (' Example: Runge-Kutta methods  ', 2, None, '___sec67'),
              (' Remark about using the $\\theta$-rule in Odespy ',
               3,
               None,
               '___sec68'),
              (' Example: Adaptive Runge-Kutta methods  ',
               2,
               'decay:fd2:adaptiveRK',
               'decay:fd2:adaptiveRK'),
              (' Exercises ', 1, None, '___sec70'),
              (' Exercise 3: Experiment with precision in tests and the size of $u$ ',
               2,
               'decay:fd2:exer:precision',
               'decay:fd2:exer:precision'),
              (' Exercise 4: Implement the 2-step backward scheme ',
               2,
               'decay:fd2:exer:bw2',
               'decay:fd2:exer:bw2'),
              (' Exercise 5: Implement the 2nd-order Adams-Bashforth scheme ',
               2,
               'decay:fd2:exer:AB2',
               'decay:fd2:exer:AB2'),
              (' Exercise 6: Implement the 3rd-order Adams-Bashforth scheme ',
               2,
               'decay:fd2:exer:AB3',
               'decay:fd2:exer:AB3'),
              (' Exercise 7: Analyze explicit 2nd-order methods ',
               2,
               'decay:exer:RK2:Taylor:analysis',
               'decay:exer:RK2:Taylor:analysis'),
              (' Problem 8: Implement and investigate the Leapfrog scheme ',
               2,
               'decay:fd2:exer:leapfrog1',
               'decay:fd2:exer:leapfrog1'),
              (' Problem 9: Make a unified implementation of many schemes ',
               2,
               'decay:fd2:exer:uni',
               'decay:fd2:exer:uni'),
              (' Applications of exponential decay models ',
               1,
               'decay:app',
               'decay:app'),
              (' Scaling ', 2, 'decay:app:scaling', 'decay:app:scaling'),
              (' Evolution of a population ',
               2,
               'decay:app:pop',
               'decay:app:pop'),
              (' Compound interest and inflation ',
               2,
               'decay:app:interest',
               'decay:app:interest'),
              (' Radioactive Decay ',
               2,
               'decay:app:nuclear',
               'decay:app:nuclear'),
              (' Deterministic model ', 3, None, '___sec83'),
              (' Stochastic model ', 3, None, '___sec84'),
              (' Relation between stochastic and deterministic models ',
               3,
               None,
               '___sec85'),
              (" Newton's law of cooling ",
               2,
               'decay:app:Newton:cooling',
               'decay:app:Newton:cooling'),
              (' Decay of atmospheric pressure with altitude ',
               2,
               'decay:app:atm',
               'decay:app:atm'),
              (' Multiple atmospheric layers ', 3, None, '___sec88'),
              (' Simplification: $L=0$ ', 3, None, '___sec89'),
              (' Simplification: one-layer model ', 3, None, '___sec90'),
              (' Compaction of sediments ',
               2,
               'decay:app:sediment',
               'decay:app:sediment'),
              (' Vertical motion of a body in a viscous fluid ',
               2,
               'decay:app:drag',
               'decay:app:drag'),
              (' Overview of forces ', 3, None, '___sec93'),
              (' Equation of motion ', 3, None, '___sec94'),
              (' Terminal velocity ', 3, None, '___sec95'),
              (' A Crank-Nicolson scheme ', 3, None, '___sec96'),
              (' Physical data ', 3, None, '___sec97'),
              (' Verification ', 3, None, '___sec98'),
              (' Scaling ', 3, None, '___sec99'),
              (' Decay ODEs from solving a PDE by Fourier expansions ',
               2,
               'decay:app:diffusion:Fourier',
               'decay:app:diffusion:Fourier'),
              (' Exercises ', 1, None, '___sec101'),
              (" Exercise 10: Derive schemes for Newton's law of cooling ",
               2,
               'decay:app:exer:cooling:schemes',
               'decay:app:exer:cooling:schemes'),
              (" Exercise 11: Implement schemes for Newton's law of cooling ",
               2,
               'decay:app:exer:cooling:py',
               'decay:app:exer:cooling:py'),
              (' Exercise 12: Find time of murder from body temperature ',
               2,
               'decay:app:exer:cooling:murder',
               'decay:app:exer:cooling:murder'),
              (' Exercise 13: Simulate an oscillating cooling process ',
               2,
               'decay:app:exer:cooling:osc',
               'decay:app:exer:cooling:osc'),
              (' Exercise 14: Radioactive decay of Carbon-14 ',
               2,
               'decay:app:exer:radio:C14',
               'decay:app:exer:radio:C14'),
              (' Exercise 15: Simulate stochastic radioactive decay ',
               2,
               'decay:app:exer:stoch:nuclear',
               'decay:app:exer:stoch:nuclear'),
              (' Exercise 16: Radioactive decay of two substances ',
               2,
               'decay:app:exer:radio:twosubst',
               'decay:app:exer:radio:twosubst'),
              (' Exercise 17: Simulate the pressure drop in the atmosphere ',
               2,
               'decay:app:exer:atm1',
               'decay:app:exer:atm1'),
              (' Exercise 18: Make a program for vertical motion in a fluid ',
               2,
               'decay:app:exer:drag:prog',
               'decay:app:exer:drag:prog'),
              (' Project 19: Simulate parachuting ',
               2,
               'decay:app:exer:parachute',
               'decay:app:exer:parachute'),
              (' Exercise 20: Formulate vertical motion in the atmosphere ',
               2,
               'decay:app:exer:drag:atm1',
               'decay:app:exer:drag:atm1'),
              (' Exercise 21: Simulate vertical motion in the atmosphere ',
               2,
               'decay:app:exer:drag:atm2',
               'decay:app:exer:drag:atm2'),
              (' Exercise 22: Compute $y=|x|$ by solving an ODE ',
               2,
               'decay:app:exer:signum',
               'decay:app:exer:signum'),
              (' Exercise 23: Simulate growth of a fortune with random interest rate ',
               2,
               'decay:app:exer:interest',
               'decay:app:exer:interest'),
              (' Exercise 24: Simulate a population in a changing environment ',
               2,
               'decay:app:exer:pop:at',
               'decay:app:exer:pop:at'),
              (' Exercise 25: Simulate logistic growth ',
               2,
               'decay:app:exer:pop:logistic1',
               'decay:app:exer:pop:logistic1'),
              (' Exercise 26: Rederive the equation for continuous compound interest ',
               2,
               'decay:app:exer:interest:derive',
               'decay:app:exer:interest:derive'),
              (' Bibliography ', 1, None, '___sec119')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript"
 src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<!-- newcommands_keep.tex -->
$$
\newcommand{\uex}{{u_{\small\mbox{e}}}}
\newcommand{\Aex}{{A_{\small\mbox{e}}}}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\tp}{\thinspace .}
\newcommand{\E}[1]{\hbox{E}\lbrack #1 \rbrack}
\newcommand{\Var}[1]{\hbox{Var}\lbrack #1 \rbrack}
\newcommand{\Std}[1]{\hbox{Std}\lbrack #1 \rbrack}
\newcommand{\Oof}[1]{\mathcal{O}(#1)}
$$




    
<a name="part0003"></a>
<p>
<!-- begin top navigation -->
<table style="width: 100%"><tr><td>
<div style="text-align: left;"><a href="._main_decay-solarized002.html">&laquo; Previous</a></div>
</td><td>
<div style="text-align: right;"><a href="._main_decay-solarized004.html">Next &raquo;</a></div>
</td></tr></table>
<!-- end top navigation -->
</p>

<p>
<!-- !split -->

<h1>Analysis of finite difference equations <a name="decay:analysis"></a></h1>

We address the ODE for exponential decay,
$$
\begin{equation}
u'(t) = -au(t),\quad u(0)=I,
\end{equation}
$$

where \( a \) and \( I \) are given constants. This problem is solved
by the \( \theta \)-rule finite difference scheme, resulting in
the recursive equations
$$
\begin{equation}
u^{n+1} = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}u^n
\tag{40}
\end{equation}
$$

for the numerical solution \( u^{n+1} \), which approximates the exact
solution \( \uex \) at time point \( t_{n+1} \). For constant mesh spacing,
which we assume here, \( t_{n+1}=(n+1)\Delta t \).

<p>
<!-- Cannot use === because it fools sphinx -->

<p>
<b>Discouraging numerical solutions.</b>
Choosing \( I=1 \), \( a=2 \), and running experiments with \( \theta =1,0.5, 0 \)
for \( \Delta t=1.25, 0.75, 0.5, 0.1 \), gives the results in
Figures <a href="#decay:analysis:BE4c">9</a>, <a href="#decay:analysis:CN4c">10</a>, and
<a href="#decay:analysis:FE4c">11</a>.

<p>
<center> <!-- figure -->
<hr class="figure">
<center><p class="caption">Figure 9:  Backward Euler. <a name="decay:analysis:BE4c"></a> </p></center>
<p><img src="fig-decay/BE4c.png" align="bottom" width=600,></p>
</center>

<p>
<center> <!-- figure -->
<hr class="figure">
<center><p class="caption">Figure 10:  Crank-Nicolson. <a name="decay:analysis:CN4c"></a> </p></center>
<p><img src="fig-decay/CN4c.png" align="bottom" width=600,></p>
</center>

<p>
<center> <!-- figure -->
<hr class="figure">
<center><p class="caption">Figure 11:  Forward Euler. <a name="decay:analysis:FE4c"></a> </p></center>
<p><img src="fig-decay/FE4c.png" align="bottom" width=600,></p>
</center>

<p>
The characteristics of the displayed curves can be summarized as follows:

<ul>
  <li> The Backward Euler scheme always gives a monotone solution, lying above
    the exact curve.</li>
  <li> The Crank-Nicolson scheme gives the most accurate results, but for
    \( \Delta t=1.25 \) the solution oscillates.</li>
  <li> The Forward Euler scheme gives a growing, oscillating solution for
    \( \Delta t=1.25 \); a decaying, oscillating solution for \( \Delta t=0.75 \);
    a strange solution \( u^n=0 \) for \( n\geq 1 \) when \( \Delta t=0.5 \); and
    a solution seemingly as accurate as the one by the Backward Euler
    scheme for \( \Delta t = 0.1 \), but the curve lies below the exact
    solution.</li>
</ul>

Since the exact solution of our model problem is a monotone function,
\( u(t)=Ie^{-at} \), some of these qualitatively wrong results are indeed alarming!

<p>
<div class="alert alert-block alert-notice alert-text-normal">
<b>Goal.</b>
<p>
We ask the question

<ul>
  <li> Under what circumstances, i.e., values of
    the input data \( I \), \( a \), and \( \Delta t \) will the Forward Euler and
    Crank-Nicolson schemes result in undesired oscillatory solutions?</li>
</ul>

The question will be investigated both by numerical experiments and
by precise mathematical theory. The latter will help establish
general criteria on \( \Delta t \) for avoiding non-physical oscillatory
or growing solutions.

<p>
Another question to be raised is

<ul>
 <li> How does \( \Delta t \) impact the error in the numerical solution?</li>
</ul>

For our simple model problem we can answer this question very precisely, but
we will also look at simplified formulas for small \( \Delta t \)
and touch upon important concepts such as <em>convergence rate</em> and
<em>the order of a scheme</em>. Other fundamental concepts mentioned are
stability, consistency, and convergence.
</div>


<h2>Experimental investigation of oscillatory solutions  <a name="___sec31"></a></h2>

To address the first question above,
we may set up an experiment where we loop over values of \( I \), \( a \),
and \( \Delta t \). For each experiment, we flag the solution as
oscillatory if
$$ u^{n} > u^{n-1},$$

for some value of \( n \),
since we expect \( u^n \) to decay with \( n \), but oscillations make
\( u \) increase over a time step. We will quickly see that
oscillations are independent of \( I \), but do depend on \( a \) and
\( \Delta t \). Therefore, we introduce a two-dimensional
function \( B(a,\Delta t) \) which is 1 if oscillations occur
and 0 otherwise. We can visualize \( B \) as a contour plot
(lines for which \( B=\hbox{const} \)). The contour \( B=0.5 \)
corresponds to the borderline between oscillatory regions with \( B=1 \)
and monotone regions with \( B=0 \) in the \( a,\Delta t \) plane.

<p>
The \( B \) function is defined at discrete \( a \) and \( \Delta t \) values.
Say we have given \( P \) $a$ values, \( a_0,\ldots,a_{P-1} \), and
\( Q \) $\Delta t$ values, \( \Delta t_0,\ldots,\Delta t_{Q-1} \).
These \( a_i \) and \( \Delta t_j \) values, \( i=0,\ldots,P-1 \),
\( j=0,\ldots,Q-1 \), form a rectangular mesh of \( P\times Q \) points
in the plane. At each point \( (a_i, \Delta t_j) \), we associate
the corresponding value of \( B(a_i,\Delta t_j) \), denoted \( B_{ij} \).
The \( B_{ij} \) values are naturally stored in a two-dimensional
array. We can thereafter create a plot of the
contour line \( B_{ij}=0.5 \) dividing the oscillatory and monotone
regions. The file <a href="http://tinyurl.com/jvzzcfn/decay/decay_osc_regions.py" target="_self"><tt>decay_osc_regions.py</tt></a>  <code>osc_regions</code> stands for "oscillatory regions") contains all nuts and
bolts to produce the \( B=0.5 \) line in Figures <a href="#decay:analysis:B:FE">12</a>
and <a href="#decay:analysis:B:CN">13</a>. The oscillatory region is above this line.

<p>

<!-- code=python (!bc pypro) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">decay_mod</span> <span style="color: #8B008B; font-weight: bold">import</span> solver
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">np</span>
<span style="color: #8B008B; font-weight: bold">import</span> <span style="color: #008b45; text-decoration: underline">scitools.std</span> <span style="color: #8B008B; font-weight: bold">as</span> <span style="color: #008b45; text-decoration: underline">st</span>

<span style="color: #8B008B; font-weight: bold">def</span> <span style="color: #008b45">non_physical_behavior</span>(I, a, T, dt, theta):
    <span style="color: #CD5555">&quot;&quot;&quot;</span>
<span style="color: #CD5555">    Given lists/arrays a and dt, and numbers I, dt, and theta,</span>
<span style="color: #CD5555">    make a two-dimensional contour line B=0.5, where B=1&gt;0.5</span>
<span style="color: #CD5555">    means oscillatory (unstable) solution, and B=0&lt;0.5 means</span>
<span style="color: #CD5555">    monotone solution of u&#39;=-au.</span>
<span style="color: #CD5555">    &quot;&quot;&quot;</span>
    a = np.asarray(a); dt = np.asarray(dt)  <span style="color: #228B22"># must be arrays</span>
    B = np.zeros((<span style="color: #658b00">len</span>(a), <span style="color: #658b00">len</span>(dt)))         <span style="color: #228B22"># results</span>
    <span style="color: #8B008B; font-weight: bold">for</span> i <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #658b00">len</span>(a)):
        <span style="color: #8B008B; font-weight: bold">for</span> j <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #658b00">len</span>(dt)):
            u, t = solver(I, a[i], T, dt[j], theta)
            <span style="color: #228B22"># Does u have the right monotone decay properties?</span>
            correct_qualitative_behavior = <span style="color: #658b00">True</span>
            <span style="color: #8B008B; font-weight: bold">for</span> n <span style="color: #8B008B">in</span> <span style="color: #658b00">range</span>(<span style="color: #B452CD">1</span>, <span style="color: #658b00">len</span>(u)):
                <span style="color: #8B008B; font-weight: bold">if</span> u[n] &gt; u[n-<span style="color: #B452CD">1</span>]:  <span style="color: #228B22"># Not decaying?</span>
                    correct_qualitative_behavior = <span style="color: #658b00">False</span>
                    <span style="color: #8B008B; font-weight: bold">break</span>  <span style="color: #228B22"># Jump out of loop</span>
            B[i,j] = <span style="color: #658b00">float</span>(correct_qualitative_behavior)
    a_, dt_ = st.ndgrid(a, dt)  <span style="color: #228B22"># make mesh of a and dt values</span>
    st.contour(a_, dt_, B, <span style="color: #B452CD">1</span>)
    st.grid(<span style="color: #CD5555">&#39;on&#39;</span>)
    st.title(<span style="color: #CD5555">&#39;theta=%g&#39;</span> % theta)
    st.xlabel(<span style="color: #CD5555">&#39;a&#39;</span>); st.ylabel(<span style="color: #CD5555">&#39;dt&#39;</span>)
    st.savefig(<span style="color: #CD5555">&#39;osc_region_theta_%s.png&#39;</span> % theta)
    st.savefig(<span style="color: #CD5555">&#39;osc_region_theta_%s.pdf&#39;</span> % theta)

non_physical_behavior(
    I=<span style="color: #B452CD">1</span>,
    a=np.linspace(<span style="color: #B452CD">0.01</span>, <span style="color: #B452CD">4</span>, <span style="color: #B452CD">22</span>),
    dt=np.linspace(<span style="color: #B452CD">0.01</span>, <span style="color: #B452CD">4</span>, <span style="color: #B452CD">22</span>),
    T=<span style="color: #B452CD">6</span>,
    theta=<span style="color: #B452CD">0.5</span>)
</pre></div>
<p>
<center> <!-- figure -->
<hr class="figure">
<center><p class="caption">Figure 12:  Forward Euler scheme: oscillatory solutions occur for points above the curve. <a name="decay:analysis:B:FE"></a> </p></center>
<p><img src="fig-decay/osc_region_FE.png" align="bottom" width=500></p>
</center>

<p>
<center> <!-- figure -->
<hr class="figure">
<center><p class="caption">Figure 13:  Crank-Nicolson scheme: oscillatory solutions occur for points above the curve. <a name="decay:analysis:B:CN"></a> </p></center>
<p><img src="fig-decay/osc_region_CN.png" align="bottom" width=500></p>
</center>

<p>
By looking at the curves in the figures one may guess that \( a\Delta t \)
must be less than a critical limit to avoid the undesired
oscillations.  This limit seems to be about 2 for Crank-Nicolson and 1
for Forward Euler.  We shall now establish a precise mathematical
analysis of the discrete model that can explain the observations in
our numerical experiments.

<h2>Exact numerical solution  <a name="___sec32"></a></h2>

Starting with \( u^0=I \), the simple recursion <a href="#mjx-eqn-40">(40)</a>
can be applied repeatedly \( n \) times, with the result that
$$
\begin{equation}
u^{n} = IA^n,\quad A = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}\tp
\tag{41}
\end{equation}
$$


<p>
<div class="alert alert-block alert-notice alert-text-normal">
<b>Solving difference equations.</b>
<p>
Difference equations where all terms are linear in
\( u^{n+1} \), \( u^n \), and maybe \( u^{n-1} \), \( u^{n-2} \), etc., are
called <em>homogeneous, linear</em> difference equations, and their solutions
are generally of the form \( u^n=A^n \). Inserting this expression
and dividing by \( A^{n+1} \) gives
a polynomial equation in \( A \). In the present case we get
$$ A = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}\tp $$

This is a solution technique of wider applicability than repeated use of
the recursion <a href="#mjx-eqn-40">(40)</a>.
</div>


<p>
Regardless of the solution approach, we have obtained a formula for
\( u^n \).  This formula can explain everything what we see in the figures
above, but it also gives us a more general insight into accuracy and
stability properties of the three schemes.

<h2>Stability  <a name="___sec33"></a></h2>

Since \( u^n \) is a factor \( A \)
raised to an integer power \( n \), we realize that \( A<0 \)
will for odd powers imply \( u^n<0 \) and for even power result in \( u^n>0 \).
That is, the solution oscillates between the mesh points.
We have oscillations due to \( A<0 \) when

$$
\begin{equation}
(1-\theta)a\Delta t > 1 \tp
\tag{42}
\end{equation}
$$

Since \( A>0 \) is a requirement for having a numerical solution with the
same basic property (monotonicity) as the exact solution, we may say
that \( A>0 \) is a <em>stability criterion</em>. Expressed in terms of \( \Delta t \)
the stability criterion reads

$$
\begin{equation}
\Delta t < \frac{1}{(1-\theta)a}\tp
\end{equation}
$$


<p>
The Backward
Euler scheme is always stable since \( A<0 \) is impossible for \( \theta=1 \), while
non-oscillating solutions for Forward Euler and Crank-Nicolson
demand \( \Delta t\leq 1/a \) and \( \Delta t\leq 2/a \), respectively.
The relation between \( \Delta t \) and \( a \) look reasonable: a larger
\( a \) means faster decay and hence a need for smaller time steps.

<p>
Looking at Figure <a href="#decay:analysis:FE4c">11</a>, we see that with \( a\Delta
t= 2\cdot 1.25=2.5 \), \( A=-1.5 \), and the solution \( u^n=(-1.5)^n \)
oscillates <em>and</em> grows. With \( a\Delta t = 2\cdot 0.75=1.5 \), \( A=-0.5 \),
\( u^n=(-0.5)^n \) decays but oscillates. The peculiar case \( \Delta t =
0.5 \), where the Forward Euler scheme produces a solution that is stuck
on the \( t \) axis, corresponds to \( A=0 \) and therefore \( u^0=I=1 \) and
\( u^n=0 \) for \( n\geq 1 \).  The decaying oscillations in the Crank-Nicolson scheme
for \( \Delta t=1.25 \) are easily explained by the fact that \( A\approx -0.11<0 \).

<p>
The factor \( A \) is called the <em>amplification factor</em> since the solution
at a new time level is \( A \) times the solution at the previous time
level. For a decay process, we must obviously have \( |A|\leq 1 \), which
is fulfilled for all \( \Delta t \) if \( \theta \geq 1/2 \). Arbitrarily
large values of \( u \) can be generated when \( |A|>1 \) and \( n \) is large
enough. The numerical solution is in such cases totally irrelevant to
an ODE modeling decay processes! To avoid this situation, we must
for \( \theta < 1/2 \) have

$$
\begin{equation}
\Delta t \leq \frac{2}{(1-2\theta)a},
\end{equation}
$$

which means \( \Delta t < 2/a \) for the Forward Euler scheme.

<p>
<div class="alert alert-block alert-notice alert-text-normal">
<b>Stability properties.</b>
<p>
We may summarize the stability investigations as follows:

<ol>
<li> The Forward Euler method is a <em>conditionally stable</em> scheme because
   it requires \( \Delta t < 2/a \) for avoiding growing solutions
   and \( \Delta t < 1/a \) for avoiding oscillatory solutions.</li>
<li> The Crank-Nicolson is <em>unconditionally stable</em> with respect to
   growing solutions, while it is conditionally stable with
   the criterion \( \Delta t < 2/a \) for avoiding oscillatory solutions.</li>
<li> The Backward Euler method is unconditionally stable with respect
   to growing and oscillatory solutions - any \( \Delta t \) will work.</li>
</ol>

Much literature on ODEs speaks about L-stable and A-stable methods.
In our case A-stable methods ensures non-growing solutions, while
L-stable methods also avoids oscillatory solutions.
</div>


<h2>Comparing amplification factors  <a name="___sec34"></a></h2>

After establishing how \( A \) impacts the qualitative features of the
solution, we shall now look more into how well the numerical amplification
factor approximates the exact one. The exact solution reads
\( u(t)=Ie^{-at} \), which can be rewritten as
$$
\begin{equation}
{\uex}(t_n) = Ie^{-a n\Delta t} = I(e^{-a\Delta t})^n \tp
\end{equation}
$$

From this formula we see that the exact amplification factor is
$$
\begin{equation}
\Aex = e^{-a\Delta t} \tp
\end{equation}
$$


<p>
We realize that the exact and numerical amplification factors depend
on \( a \) and \( \Delta t \) through the product \( a\Delta t \). Therefore, it
is convenient to introduce a symbol for this product, \( p=a\Delta t \),
and view \( A \) and \( \Aex \) as functions of \( p \). Figure
<a href="#decay:analysis:fig:A">14</a> shows these functions. Crank-Nicolson is
clearly closest to the exact amplification factor, but that method has
the unfortunate oscillatory behavior when \( p>2 \).

<p>
<center> <!-- figure -->
<hr class="figure">
<center><p class="caption">Figure 14:  Comparison of amplification factors. <a name="decay:analysis:fig:A"></a> </p></center>
<p><img src="fig-decay/A_factors.png" align="bottom" width=500></p>
</center>

<h2>Series expansion of amplification factors  <a name="___sec35"></a></h2>

As an alternative to the visual understanding inherent in Figure
<a href="#decay:analysis:fig:A">14</a>, there is a strong tradition in numerical
analysis to establish formulas for the approximation errors when the
discretization parameter, here \( \Delta t \), becomes small. In the
present case we let \( p \) be our small discretization parameter, and it
makes sense to simplify the expressions for \( A \) and \( \Aex \) by using
Taylor polynomials around \( p=0 \).  The Taylor polynomials are accurate
for small \( p \) and greatly simplifies the comparison of the analytical
expressions since we then can compare polynomials, term by term.

<p>
Calculating the Taylor series for \( \Aex \) is easily done by hand, but
the three versions of \( A \) for \( \theta=0,1,{\half} \) lead to more
cumbersome calculations.
Nowadays, analytical computations can benefit greatly by
symbolic computer algebra software. The Python package <code>sympy</code>
represents a powerful computer algebra system, not yet as sophisticated as
the famous Maple and Mathematica systems, but free and
very easy to integrate with our numerical computations in Python.

<p>
When using <code>sympy</code>, it is convenient to enter the interactive Python
mode where we can write expressions and statements and immediately see
the results.  Here is a simple example. We strongly recommend to use
<code>isympy</code> (or <code>ipython</code>) for such interactive sessions.

<p>
Let us illustrate <code>sympy</code> with a standard Python shell syntax
(<code>>>></code> prompt) to compute a Taylor polynomial approximation to \( e^{-p} \):

<p>

<!-- code=ipy (!bc ipy) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%">&gt;&gt;&gt; <span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">sympy</span> <span style="color: #8B008B; font-weight: bold">import</span> *
&gt;&gt;&gt; <span style="color: #228B22"># Create p as a mathematical symbol with name &#39;p&#39;</span>
&gt;&gt;&gt; p = Symbol(<span style="color: #CD5555">&#39;p&#39;</span>)
&gt;&gt;&gt; <span style="color: #228B22"># Create a mathematical expression with p</span>
&gt;&gt;&gt; A_e = exp(-p)
&gt;&gt;&gt;
&gt;&gt;&gt; <span style="color: #228B22"># Find the first 6 terms of the Taylor series of A_e</span>
&gt;&gt;&gt; A_e.series(p, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">6</span>)
<span style="color: #B452CD">1</span> + (<span style="color: #B452CD">1</span>/<span style="color: #B452CD">2</span>)*p**<span style="color: #B452CD">2</span> - p - <span style="color: #B452CD">1</span>/<span style="color: #B452CD">6</span>*p**<span style="color: #B452CD">3</span> - <span style="color: #B452CD">1</span>/<span style="color: #B452CD">120</span>*p**<span style="color: #B452CD">5</span> + (<span style="color: #B452CD">1</span>/<span style="color: #B452CD">24</span>)*p**<span style="color: #B452CD">4</span> + O(p**<span style="color: #B452CD">6</span>)
</pre></div>
<p>
Lines with <code>>>></code> represent input lines and lines without
this prompt represents the result of computations (note that
<code>isympy</code> and <code>ipython</code> apply other prompts, but in this text
we always apply <code>>>></code> for interactive Python computing).
Apart from the order of the powers, the computed formula is easily
recognized as the beginning of the Taylor series for \( e^{-p} \).

<p>
Let us define the numerical amplification factor where \( p \) and \( \theta \)
enter the formula as symbols:
<p>

<!-- code=ipy (!bc ipy) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%">&gt;&gt;&gt; theta = Symbol(<span style="color: #CD5555">&#39;theta&#39;</span>)
&gt;&gt;&gt; A = (<span style="color: #B452CD">1</span>-(<span style="color: #B452CD">1</span>-theta)*p)/(<span style="color: #B452CD">1</span>+theta*p)
</pre></div>
<p>
To work with the factor for the Backward Euler scheme we
can substitute the value 1 for <code>theta</code>:
<p>

<!-- code=ipy (!bc ipy) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%">&gt;&gt;&gt; A.subs(theta, <span style="color: #B452CD">1</span>)
<span style="color: #B452CD">1</span>/(<span style="color: #B452CD">1</span> + p)
</pre></div>
<p>
Similarly, we can replace <code>theta</code> by 1/2 for Crank-Nicolson,
preferably using an exact rational representation of 1/2 in <code>sympy</code>:
<p>

<!-- code=ipy (!bc ipy) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%">&gt;&gt;&gt; half = Rational(<span style="color: #B452CD">1</span>,<span style="color: #B452CD">2</span>)
&gt;&gt;&gt; A.subs(theta, half)
<span style="color: #B452CD">1</span>/(<span style="color: #B452CD">1</span> + (<span style="color: #B452CD">1</span>/<span style="color: #B452CD">2</span>)*p)*(<span style="color: #B452CD">1</span> - <span style="color: #B452CD">1</span>/<span style="color: #B452CD">2</span>*p)
</pre></div>
<p>
The Taylor series of the amplification factor for the Crank-Nicolson
scheme can be computed as
<p>

<!-- code=ipy (!bc ipy) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%">&gt;&gt;&gt; A.subs(theta, half).series(p, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">4</span>)
<span style="color: #B452CD">1</span> + (<span style="color: #B452CD">1</span>/<span style="color: #B452CD">2</span>)*p**<span style="color: #B452CD">2</span> - p - <span style="color: #B452CD">1</span>/<span style="color: #B452CD">4</span>*p**<span style="color: #B452CD">3</span> + O(p**<span style="color: #B452CD">4</span>)
</pre></div>
<p>
We are now in a position to compare Taylor series:
<p>

<!-- code=ipy (!bc ipy) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%">&gt;&gt;&gt; FE = A_e.series(p, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">4</span>) - A.subs(theta, <span style="color: #B452CD">0</span>).series(p, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">4</span>)
&gt;&gt;&gt; BE = A_e.series(p, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">4</span>) - A.subs(theta, <span style="color: #B452CD">1</span>).series(p, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">4</span>)
&gt;&gt;&gt; CN = A_e.series(p, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">4</span>) - A.subs(theta, half).series(p, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">4</span> )
&gt;&gt;&gt; FE
(<span style="color: #B452CD">1</span>/<span style="color: #B452CD">2</span>)*p**<span style="color: #B452CD">2</span> - <span style="color: #B452CD">1</span>/<span style="color: #B452CD">6</span>*p**<span style="color: #B452CD">3</span> + O(p**<span style="color: #B452CD">4</span>)
&gt;&gt;&gt; BE
-<span style="color: #B452CD">1</span>/<span style="color: #B452CD">2</span>*p**<span style="color: #B452CD">2</span> + (<span style="color: #B452CD">5</span>/<span style="color: #B452CD">6</span>)*p**<span style="color: #B452CD">3</span> + O(p**<span style="color: #B452CD">4</span>)
&gt;&gt;&gt; CN
(<span style="color: #B452CD">1</span>/<span style="color: #B452CD">12</span>)*p**<span style="color: #B452CD">3</span> + O(p**<span style="color: #B452CD">4</span>)
</pre></div>
<p>
From these expressions we see that the error \( A-\Aex\sim \Oof{p^2} \)
for the Forward and Backward Euler schemes, while
\( A-\Aex\sim \Oof{p^3} \) for the Crank-Nicolson scheme.
It is the <em>leading order term</em>,
i.e., the term of the lowest order (polynomial degree),
that is of interest, because as \( p\rightarrow 0 \), this term is
(much) bigger than the higher-order terms (think of \( p=0.01 \):
\( p \) is a hundred times larger than \( p^2 \)).

<p>
Now, \( a \) is a given parameter in the problem, while \( \Delta t \) is
what we can vary. One therefore usually writes the error expressions in
terms \( \Delta t \). When then have
$$
\begin{equation}
A-\Aex = \left\lbrace\begin{array}{ll}
\Oof{\Delta t^2}, & \hbox{Forward and Backward Euler},\\ 
\Oof{\Delta t^3}, & \hbox{Crank-Nicolson}
\end{array}\right.
\end{equation}
$$


<p>
We say that the Crank-Nicolson scheme has an error in the amplification
factor of order \( \Delta t^3 \), while the two other schemes are
of order \( \Delta t^2 \) in the same quantity.
What is the significance of the order expression? If we halve \( \Delta t \),
the error in amplification factor at a time level will be reduced
by a factor of 4 in the Forward and Backward Euler schemes, and by
a factor of 8 in the Crank-Nicolson scheme. That is, as we
reduce \( \Delta t \) to obtain more accurate results, the Crank-Nicolson
scheme reduces the error more efficiently than the other schemes.

<h2>The fraction of numerical and exact amplification factors  <a name="___sec36"></a></h2>

An alternative comparison of the schemes is to look at the
ratio \( A/\Aex \), or the error \( 1-A/\Aex \) in this ratio:
<p>

<!-- code=ipy (!bc ipy) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%">&gt;&gt;&gt; FE = <span style="color: #B452CD">1</span> - (A.subs(theta, <span style="color: #B452CD">0</span>)/A_e).series(p, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">4</span>)
&gt;&gt;&gt; BE = <span style="color: #B452CD">1</span> - (A.subs(theta, <span style="color: #B452CD">1</span>)/A_e).series(p, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">4</span>)
&gt;&gt;&gt; CN = <span style="color: #B452CD">1</span> - (A.subs(theta, half)/A_e).series(p, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">4</span>)
&gt;&gt;&gt; FE
(<span style="color: #B452CD">1</span>/<span style="color: #B452CD">2</span>)*p**<span style="color: #B452CD">2</span> + (<span style="color: #B452CD">1</span>/<span style="color: #B452CD">3</span>)*p**<span style="color: #B452CD">3</span> + O(p**<span style="color: #B452CD">4</span>)
&gt;&gt;&gt; BE
-<span style="color: #B452CD">1</span>/<span style="color: #B452CD">2</span>*p**<span style="color: #B452CD">2</span> + (<span style="color: #B452CD">1</span>/<span style="color: #B452CD">3</span>)*p**<span style="color: #B452CD">3</span> + O(p**<span style="color: #B452CD">4</span>)
&gt;&gt;&gt; CN
(<span style="color: #B452CD">1</span>/<span style="color: #B452CD">12</span>)*p**<span style="color: #B452CD">3</span> + O(p**<span style="color: #B452CD">4</span>)
</pre></div>
<p>
The leading-order terms have the same powers as
in the analysis of \( A-\Aex \).

<h2>The global error at a point <a name="decay:analysis:gobal:error"></a></h2>

The error in the amplification factor reflects the error when
progressing from time level \( t_n \) to \( t_{n-1} \).
To investigate the real error at a point, known as the <em>global error</em>,
we look at \( e^n = u^n-\uex(t_n) \) for some \( n \) and Taylor expand the
mathematical expressions as functions of \( p=a\Delta t \):
<p>

<!-- code=ipy (!bc ipy) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%">&gt;&gt;&gt; n = Symbol(<span style="color: #CD5555">&#39;n&#39;</span>)
&gt;&gt;&gt; u_e = exp(-p*n)
&gt;&gt;&gt; u_n = A**n
&gt;&gt;&gt; FE = u_e.series(p, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">4</span>) - u_n.subs(theta, <span style="color: #B452CD">0</span>).series(p, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">4</span>)
&gt;&gt;&gt; BE = u_e.series(p, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">4</span>) - u_n.subs(theta, <span style="color: #B452CD">1</span>).series(p, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">4</span>)
&gt;&gt;&gt; CN = u_e.series(p, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">4</span>) - u_n.subs(theta, half).series(p, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">4</span>)
&gt;&gt;&gt; FE
(<span style="color: #B452CD">1</span>/<span style="color: #B452CD">2</span>)*n*p**<span style="color: #B452CD">2</span> - <span style="color: #B452CD">1</span>/<span style="color: #B452CD">2</span>*n**<span style="color: #B452CD">2</span>*p**<span style="color: #B452CD">3</span> + (<span style="color: #B452CD">1</span>/<span style="color: #B452CD">3</span>)*n*p**<span style="color: #B452CD">3</span> + O(p**<span style="color: #B452CD">4</span>)
&gt;&gt;&gt; BE
(<span style="color: #B452CD">1</span>/<span style="color: #B452CD">2</span>)*n**<span style="color: #B452CD">2</span>*p**<span style="color: #B452CD">3</span> - <span style="color: #B452CD">1</span>/<span style="color: #B452CD">2</span>*n*p**<span style="color: #B452CD">2</span> + (<span style="color: #B452CD">1</span>/<span style="color: #B452CD">3</span>)*n*p**<span style="color: #B452CD">3</span> + O(p**<span style="color: #B452CD">4</span>)
&gt;&gt;&gt; CN
(<span style="color: #B452CD">1</span>/<span style="color: #B452CD">12</span>)*n*p**<span style="color: #B452CD">3</span> + O(p**<span style="color: #B452CD">4</span>)
</pre></div>
<p>
For a fixed time \( t \), the parameter \( n \) in these expressions increases
as \( p\rightarrow 0 \) since \( t=n\Delta t =\mbox{const} \) and hence
\( n \) must increase like \( \Delta t^{-1} \). With \( n \) substituted by
\( t/\Delta t \) in
the leading-order error terms, these become \( \half na^2\Delta
t^2 = {\half}ta^2\Delta t \) for the Forward and Backward Euler
scheme, and \( \frac{1}{12}na^3\Delta t^3 = \frac{1}{12}ta^3\Delta t^2 \)
for the Crank-Nicolson scheme.  The global error is therefore of
second order (in \( \Delta t \)) for the latter scheme and of first order for
the former schemes.

<p>
When the global error \( e^n\rightarrow 0 \) as \( \Delta t\rightarrow 0 \),
we say that the scheme is <em>convergent</em>. It means that the numerical
solution approaches the exact solution as the mesh is refined, and
this is a much desired property of a numerical method.

<h2>Integrated errors  <a name="___sec38"></a></h2>

It is common to study the norm of the numerical error, as
explained in detail in the section <a href="._main_decay-solarized002.html#decay:computing:error:norm">Computing the norm of the numerical error</a>.
The \( L^2 \) norm can be computed by treating \( e^n \) as a function
of \( t \) in <code>sympy</code> and performing symbolic integration. For
the Forward Euler scheme we have

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%">p, n, a, dt, t, T, theta = symbols(<span style="color: #CD5555">&#39;p n a dt t T &#39;</span>theta<span style="color: #CD5555">&#39;)</span>
A = (<span style="color: #B452CD">1</span>-(<span style="color: #B452CD">1</span>-theta)*p)/(<span style="color: #B452CD">1</span>+theta*p)
u_e = exp(-p*n)
u_n = A**n
error = u_e.series(p, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">4</span>) - u_n.subs(theta, <span style="color: #B452CD">0</span>).series(p, <span style="color: #B452CD">0</span>, <span style="color: #B452CD">4</span>)
<span style="color: #228B22"># Introduce t and dt instead of n and p</span>
error = error.subs(<span style="color: #CD5555">&#39;n&#39;</span>, <span style="color: #CD5555">&#39;t/dt&#39;</span>).subs(p, <span style="color: #CD5555">&#39;a*dt&#39;</span>)
error = error.as_leading_term(dt) <span style="color: #228B22"># study only the first term</span>
<span style="color: #8B008B; font-weight: bold">print</span> error
error_L2 = sqrt(integrate(error**<span style="color: #B452CD">2</span>, (t, <span style="color: #B452CD">0</span>, T)))
<span style="color: #8B008B; font-weight: bold">print</span> error_L2
</pre></div>
<p>
The output reads

<p>

<!-- code=text typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%">sqrt(30)*sqrt(T**3*a**4*dt**2*(6*T**2*a**2 - 15*T*a + 10))/60
</pre></div>
<p>
which means that the \( L^2 \) error behaves like \( a^2\Delta t \).

<p>
Strictly speaking, the numerical error is only defined at the
mesh points so it makes most sense to compute the
\( \ell^2 \) error

$$ ||e^n||_{\ell^2} = \sqrt{\Delta t\sum_{n=0}^{N_t} ({\uex}(t_n) - u^n)^2}
\tp $$

We have obtained an exact analytical expressions for the error at \( t=t_n \),
but here we use the leading-order error term only since we are mostly
interested in how the error behaves as a polynomial in \( \Delta t \), and then
the leading order term will dominate.
For the Forward Euler scheme,
\( \uex(t_n) - u^n \approx {\half}np^2 \), and we have

$$ ||e^n||_{\ell^2}^2 = \Delta t\sum_{n=0}^{N_t} \frac{1}{4}n^2p^4
=\Delta t\frac{1}{4}p^4 \sum_{n=0}^{N_t} n^2\tp$$

Now, \( \sum_{n=0}^{N_t} n^2\approx \frac{1}{3}N_t^3 \). Using this approximation,
setting \( N_t =T/\Delta t \), and taking the square root gives the expression

$$ ||e^n||_{\ell^2} = \half\sqrt{\frac{T^3}{3}} a^2\Delta t\tp$$

Calculations for the Backward Euler scheme are very similar and provide
the same result, while the Crank-Nicolson scheme leads to

$$ ||e^n||_{\ell^2} = \frac{1}{12}\sqrt{\frac{T^3}{3}}a^3\Delta t^2\tp$$


<p>
<div class="alert alert-block alert-summary alert-text-normal">
<b>Summary of errors.</b>
<p>
Both the point-wise and the time-integrated true errors are of
second order in \( \Delta t \) for the Crank-Nicolson scheme and of
first order in \( \Delta t \) for the Forward Euler and Backward Euler schemes.
</div>


<h2>Truncation error  <a name="___sec39"></a></h2>

The truncation error is a very frequently used error measure for
finite difference methods. It is defined as <em>the error
in the difference equation that arises when inserting the exact
solution</em>. Contrary to many other error measures, e.g., the
true error \( e^n=\uex(t_n)-u^n \), the truncation error is a quantity that
is easily computable.

<p>
Let us illustrate the calculation of the truncation error
for the Forward Euler scheme.
We start with the difference equation on operator form,

$$ \lbrack D_t u = -au\rbrack^n,$$

i.e.,

$$ \frac{u^{n+1}-u^n}{\Delta t} = -au^n\tp$$

The idea is to see how well the exact solution \( \uex(t) \) fulfills
this equation. Since \( \uex(t) \) in general will not obey the
discrete equation, error in the discrete equation, called
a <em>residual</em>, denoted here by \( R^n \):

$$
\begin{equation}
R^n = \frac{\uex(t_{n+1})-\uex(t_n)}{\Delta t} + a\uex(t_n)
\tp
\tag{43}
\end{equation}
$$

The residual is defined at each mesh point and is therefore a mesh
function with a superscript \( n \).

<p>
The interesting feature of \( R^n \) is to see how it
depends on the discretization parameter \( \Delta t \).
The tool for reaching
this goal is to Taylor expand \( \uex \) around the point where the
difference equation is supposed to hold, here \( t=t_n \).
We have that

$$ \uex(t_{n+1}) = \uex(t_n) + \uex'(t_n)\Delta t + \half\uex''(t_n)
\Delta t^2 + \cdots $$

Inserting this Taylor series in <a href="#mjx-eqn-43">(43)</a> gives

$$ R^n = \uex'(t_n) + \half\uex''(t_n)\Delta t + \ldots + a\uex(t_n)\tp$$

Now, \( \uex \) fulfills the ODE \( \uex'=-a\uex \) such that the first and last
term cancels and we have

$$ R^n \approx \half\uex''(t_n)\Delta t \tp $$

This \( R^n \) is the <em>truncation error</em>, which for the Forward Euler is seen
to be of first order in \( \Delta t \).

<p>
The above procedure can be repeated for the Backward Euler and the
Crank-Nicolson schemes. We start with the scheme in operator notation,
write it out in detail, Taylor expand \( \uex \) around the point \( \tilde t \)
at which the difference equation is defined, collect terms that
correspond to the ODE (here \( \uex' + a\uex \)), and identify the remaining
terms as the residual \( R \), which is the truncation error.
The Backward Euler scheme leads to

$$ R^n \approx -\half\uex''(t_n)\Delta t, $$

while the Crank-Nicolson scheme gives

$$ R^{n+\half} \approx \frac{1}{24}\uex'''(t_{n+\half})\Delta t^2\tp$$


<p>
The <em>order</em> \( r \) of a finite difference scheme is often defined through
the leading term \( \Delta t^r \) in the truncation error. The above
expressions point out that the Forward and Backward Euler schemes are
of first order, while Crank-Nicolson is of second order.  We have
looked at other error measures in other sections, like the error in
amplification factor and the error \( e^n=\uex(t_n)-u^n \), and expressed
these error measures in terms of \( \Delta t \) to see the order of the
method. Normally, calculating the truncation error is more
straightforward than deriving the expressions for other error measures
and therefore the easiest way to establish the order of a scheme.

<h2>Consistency, stability, and convergence  <a name="___sec40"></a></h2>

Three fundamental concepts when solving differential equations by
numerical methods are consistency, stability, and convergence.  We
shall briefly touch these concepts below in the context of the present
model problem.

<p>
Consistency means that the error in the difference equation, measured
through the truncation error, goes to zero as \( \Delta t\rightarrow
0 \). Since the truncation error tells how well the exact solution
fulfills the difference equation, and the exact solution fulfills the
differential equation, consistency ensures that the difference
equation approaches the differential equation in the limit. The
expressions for the truncation errors in the previous section are all
proportional to \( \Delta t \) or \( \Delta t^2 \), hence they vanish as
\( \Delta t\rightarrow 0 \), and all the schemes are consistent.  Lack of
consistency implies that we actually solve a different differential
equation in the limit \( \Delta t\rightarrow 0 \) than we aim at.

<p>
Stability means that the numerical solution exhibits the same
qualitative properties as the exact solution. This is obviously a
feature we want the numerical solution to have. In the present
exponential decay model, the exact solution is monotone and
decaying. An increasing numerical solution is not in accordance with
the decaying nature of the exact solution and hence unstable. We can
also say that an oscillating numerical solution lacks the property of
monotonicity of the exact solution and is also unstable. We have seen
that the Backward Euler scheme always leads to monotone and decaying
solutions, regardless of \( \Delta t \), and is hence stable. The Forward
Euler scheme can lead to increasing solutions and oscillating
solutions if \( \Delta t \) is too large and is therefore unstable unless
\( \Delta t \) is sufficiently small.  The Crank-Nicolson can never lead
to increasing solutions and has no problem to fulfill that stability
property, but it can produce oscillating solutions and is unstable in
that sense, unless \( \Delta t \) is sufficiently small.

<p>
Convergence implies that the global (true) error mesh function \( e^n =
\uex(t_n)-u^n\rightarrow 0 \) as \( \Delta t\rightarrow 0 \). This is really
what we want: the numerical solution gets as close to the exact
solution as we request by having a sufficiently fine mesh.

<p>
Convergence is hard to establish theoretically, except in quite simple
problems like the present one. Stability and consistency are much
easier to calculate. A major breakthrough in the understanding of
numerical methods for differential equations came in 1956 when Lax and
Richtmeyer established equivalence between convergence on one hand and
consistency and stability on the other (the <a href="http://en.wikipedia.org/wiki/Lax_equivalence_theorem" target="_self">Lax equivalence theorem</a>).  In practice
it meant that one can first establish that a method is stable and
consistent, and then it is automatically convergent (which is much
harder to establish).  The result holds for linear problems only, and
in the world of nonlinear differential equations the relations between
consistency, stability, and convergence are much more complicated.

<p>
We have seen in the previous analysis that the Forward Euler,
Backward Euler, and Crank-Nicolson schemes are convergent (\( e^n\rightarrow 0 \)),
that they are consistent (\( R^n\rightarrow 0 \), and that they are
stable under certain conditions on the size of \( \Delta t \).
We have also derived explicit mathematical expressions for \( e^n \),
the truncation error, and the stability criteria.

<p>
<!-- Look in Asher and Petzold, p 40 -->

<h1>Exercises  <a name="___sec41"></a></h1>

<!-- --- begin exercise --- -->

<h2>Exercise 1: Visualize the accuracy of finite differences <a name="decay:analysis:exer:fd:exp:plot"></a></h2>

The purpose of this exercise is to visualize the accuracy of finite difference
approximations of the derivative of a given function.
For any finite difference approximation, take the Forward Euler difference
as an example, and any specific function, take  \( u=e^{-at} \),
we may introduce an error fraction

$$ E = \frac{[D_t^+ u]^n}{u'(t_n)} = \frac{\exp{(-a(t_n+\Delta t))} - \exp{(-at_n)}}{-a\exp{(-at_n)\Delta t}} = \frac{1}{a\Delta t}\left(1 -\exp{(-a\Delta t)}\right),
$$

and view \( E \) as a function of \( \Delta t \). We expect that
\( \lim_{\Delta t\rightarrow 0}E=1 \), while \( E \) may deviate significantly from
unity for large \( \Delta t \). How the error depends on \( \Delta t \) is best
visualized in a graph where we use a logarithmic scale for \( \Delta t \),
so we can cover many orders of magnitude of that quantity. Here is
a code segment creating an array of 100 intervals, on the logarithmic
scale, ranging from \( 10^{-6} \) to \( 10^{-0.5} \) and then plotting \( E \) versus
\( p=a\Delta t \) with logarithmic scale on the \( p \) axis:

<p>

<!-- code=python (!bc pycod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="line-height: 125%"><span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">numpy</span> <span style="color: #8B008B; font-weight: bold">import</span> logspace, exp
<span style="color: #8B008B; font-weight: bold">from</span> <span style="color: #008b45; text-decoration: underline">matplotlib.pyplot</span> <span style="color: #8B008B; font-weight: bold">import</span> plot, semilogx
p = logspace(-<span style="color: #B452CD">6</span>, -<span style="color: #B452CD">0.5</span>, <span style="color: #B452CD">101</span>)
y = (<span style="color: #B452CD">1</span>-exp(-p))/p
semilogx(p, y)
</pre></div>
<p>
Illustrate such errors for the finite difference operators \( [D_t^+u]^n \)
(forward), \( [D_t^-u]^n \) (backward), and \( [D_t u]^n \) (centered) in
the same plot.

<p>
Perform a Taylor series expansions of the error fractions and find
the leading order \( r \) in the expressions of type
\( 1 + Cp^r + \Oof{p^{r+1}} \), where \( C \) is some constant.

<p>
<!-- --- begin hint in exercise --- -->

<p>
<b>Hint.</b>
To save manual calculations and learn more about symbolic computing,
make functions for the three difference operators and use <code>sympy</code>
to perform the symbolic differences, differentiation, and Taylor series
expansion. To plot a symbolic expression <code>E</code> against <code>p</code>, convert the
expression to a Python function first: <code>E = sympy.lamdify([p], E)</code>.

<p>
<!-- --- end hint in exercise --- -->
Filename: <code>decay_plot_fd_error.py</code>.

<p>
<!-- --- end exercise --- -->

<p>
<!-- --- begin exercise --- -->

<h2>Exercise 2: Explore the \( \theta \)-rule for exponential growth <a name="decay:analysis:exer:growth"></a></h2>

This exercise asks you to solve the ODE \( u'=-au \) with \( a<0 \) such that
the ODE models exponential growth instead of exponential decay.  A
central theme is to investigate numerical artifacts and non-physical
solution behavior.

<p>
<b>a)</b>
Set \( a=-1 \) and run experiments with \( \theta=0, 0.5, 1 \) for
various values of \( \Delta t \) to uncover numerical artifacts.
Recall that the exact solution is a
monotone, growing function when \( a<0 \). Oscillations or significantly
wrong growth are signs of wrong qualitative behavior.

<p>
From the experiments, select four values of \( \Delta t \) that
demonstrate the kind of numerical solutions that are characteristic
for this model.
Filename: <code>growth_demo.py</code>.

<p>
<b>b)</b>
Write up the amplification factor and plot it for \( \theta=0,0.5,1 \)
together with the exact one for \( a\Delta t <0 \). Use the plot to
explain the observations made in the experiments.

<p>
<!-- --- begin hint in exercise --- -->

<p>
<b>Hint.</b>
Modify the <a href="http://tinyurl.com/jvzzcfn/decay/decay_ampf_plot.py" target="_self"><tt>decay_ampf_plot.py</tt></a> code.

<p>
<!-- --- end hint in exercise --- -->
Filename: <code>growth_ampf_plot.py</code>.

<p>
<!-- --- end exercise --- -->

<p>
<p>
<!-- begin bottom navigation -->
<table style="width: 100%"><tr><td>
<div style="text-align: left;"><a href="._main_decay-solarized002.html">&laquo; Previous</a></div>
</td><td>
<div style="text-align: right;"><a href="._main_decay-solarized004.html">Next &raquo;</a></div>
</td></tr></table>
<!-- end bottom navigation -->
</p>

<!-- ------------------- end of main content --------------- -->


</body>
</html>
    

