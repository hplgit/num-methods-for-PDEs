.. !split

Multi-dimensional PDE problems
==============================

The fundamental ideas in the
derivation of :math:`F_i` and :math:`J_{i,j}` in the 1D model problem
are easily generalized to multi-dimensional problems.
Nevertheless, the expressions involved are slightly different, with
derivatives in :math:`x` replaced by :math:`\nabla`, so we present some
examples below in detail.

.. _nonlin:alglevel:dD:fe:

Finite element discretization
-----------------------------

As an example, Backward Euler discretization of the
PDE

.. _Eq:nonlin:alglevel:dD:fe:PDE:

.. math::

    \tag{75}
    u_t = \nabla\cdot({\alpha}(u)\nabla u) + f(u),
        
        

gives the nonlinear time-discrete PDEs

.. math::
         u^n - \Delta t\nabla\cdot({\alpha}(u^n)\nabla u^n) + f(u^n) = u^{n-1}{\thinspace .}

We may alternatively write this equation
with :math:`u` for :math:`u^n` and :math:`u^{(1)}` for :math:`u^{n-1}`:

.. math::
         u - \Delta t\nabla\cdot({\alpha}(u)\nabla u) - \Delta t f(u) = u^{(1)}{\thinspace .}


.. admonition:: Understand the meaning of the symbol :math:`u` in various formulas

   Note that the mathematical meaning of the symbol :math:`u` changes in
   the above equations: :math:`u(\boldsymbol{x},t)` is the exact solution of :ref:`(75) <Eq:nonlin:alglevel:dD:fe:PDE>`, :math:`u^n(\boldsymbol{x})` is an approximation to the exact solution at :math:`t=t_n`,
   while :math:`u(\boldsymbol{x})` in the latter equation is a synonym for :math:`u^n`.
   Below, this :math:`u(\boldsymbol{x})` will be approximated by a new
   :math:`u=\sum_kc_k{\psi}_k(\boldsymbol{x})` in space, and then the actual :math:`u` symbol used
   in the Picard and Newton iterations is a further approximation of
   :math:`\sum_kc_k{\psi}_k` arising from the nonlinear iteration algorithm.
   
   Much literature reserves :math:`u` for the exact solution, uses :math:`u_h(x, t)`
   for the finite element solution that varies continuously in time,
   introduces perhaps :math:`u_h^n` as the approximation of :math:`u_h` at time :math:`t_n`,
   arising from some time discretization, and then finally applies
   :math:`u_h^{n,k}` for the approximation to :math:`u_h^n` in the :math:`k`-th
   iteration of a Picard or Newton method. The converged solution at
   the previous time step can be called :math:`u_h^{n-1}`, but then this
   quantity is an approximate solution of the nonlinear equations (at the
   previous time level), while the counterpart :math:`u_h^n` is formally the
   exact solution of the nonlinear equations at the current time level.
   The various symbols in the
   mathematics can in this way be clearly distinguished. However,
   we favor to use :math:`u` for the quantity that is most naturally called ``u``
   in the code, and that is the most recent approximation to the solution,
   e.g., named :math:`u_h^{n,k}` above. This is also the key quantity of
   interest in mathematical derivations of algorithms as well.
   Choosing :math:`u` this way makes the most important mathematical cleaner
   than using more cluttered notation as :math:`u_h^{n,k}`.
   We therefore introduce other symbols for other versions of the unknown
   function. It is most appropriate for us to say that :math:`{u_{\small\mbox{e}}}(\boldsymbol{x}, t)` is the
   exact solution, :math:`u^n` in the equation above
   is the approximation to :math:`{u_{\small\mbox{e}}}(\boldsymbol{x},t_n)` after
   time discretization, and :math:`u` is the spatial approximation to
   :math:`u^n` from the most recent iteration in a nonlinear iteration method.




Let us assume homogeneous Neumann conditions
on the entire boundary for simplicity in the boundary term. The
variational form becomes: find :math:`u\in V` such that

.. _Eq:nonlin:alglevel:dD:fe:varform:

.. math::

    \tag{76}
    \int_\Omega (uv + \Delta t\,{\alpha}(u)\nabla u\cdot\nabla v
        - \Delta t f(u)v - u^{(1)} v){\, \mathrm{d}x} = 0,\quad \forall v\in V{\thinspace .}
        
        

The nonlinear algebraic equations follow from setting :math:`v={\psi}_i`
and using the representation :math:`u=\sum_kc_k{\psi}_k`, which we
just write as

.. _Eq:nonlin:alglevel:dD:fe:Fi:

.. math::

    \tag{77}
    F_i =
        \int_\Omega (u{\psi}_i + \Delta t\,{\alpha}(u)\nabla u\cdot\nabla {\psi}_i
        - \Delta t f(u){\psi}_i - u^{(1)}{\psi}_i){\, \mathrm{d}x}{\thinspace .}
        
        

Picard iteration needs a linearization where we use
the most recent approximation :math:`u^{-}` to :math:`u` in
:math:`{\alpha}` and :math:`f`:

.. _Eq:nonlin:alglevel:dD:fe:Fi:Picard:

.. math::

    \tag{78}
    F_i \approx \hat F_i =
        \int_\Omega (u{\psi}_i + \Delta t\,{\alpha}(u^{-})\nabla u\cdot\nabla {\psi}_i
        - \Delta t f(u^{-}){\psi}_i - u^{(1)}{\psi}_i){\, \mathrm{d}x}{\thinspace .}
        
        

The equations :math:`\hat F_i=0` are now linear and we can easily derive
a linear system :math:`\sum_{j\in{\mathcal{I}_s}}A_{i,j}c_j=b_i`, :math:`i\in{\mathcal{I}_s}`,
for the unknown coefficients
:math:`\left\{ {c}_i \right\}_{i\in{\mathcal{I}_s}}` by inserting :math:`u=\sum_jc_j{\psi}_j`. We get

.. math::
         A_{i,j} =
        \int_\Omega ({\varphi}_j{\psi}_i + \Delta t\,{\alpha}(u^{-})\nabla {\varphi}_j
        \cdot\nabla {\psi}_i){\, \mathrm{d}x},\quad b_i =
        \int_\Omega (\Delta t f(u^{-}){\psi}_i + u^{(1)}{\psi}_i){\, \mathrm{d}x}{\thinspace .}
        

In Newton's method we need to evaluate :math:`F_i` with the known value
:math:`u^{-}` for :math:`u`:

.. _Eq:nonlin:alglevel:dD:fe:Fi:Newton:

.. math::

    \tag{79}
    F_i \approx \hat F_i =
        \int_\Omega (u^{-}{\psi}_i + \Delta t\,{\alpha}(u^{-})
        \nabla u^{-}\cdot\nabla {\psi}_i
        - \Delta t f(u^{-}){\psi}_i - u^{(1)}{\psi}_i){\, \mathrm{d}x}{\thinspace .}
        
        

The Jacobian is obtained by differentiating
:ref:`(77) <Eq:nonlin:alglevel:dD:fe:Fi>` and using

.. _Eq:_auto33:

.. math::

    \tag{80}
    \frac{\partial u}{\partial c_j} = \sum_k\frac{\partial}{\partial c_j}
        c_k{\psi}_k = {\psi}_j,
        
        

.. _Eq:_auto34:

.. math::

    \tag{81}
    \frac{\partial \nabla u}{\partial c_j} = \sum_k\frac{\partial}{\partial c_j}
        c_k\nabla {\psi}_k = \nabla {\psi}_j{\thinspace .}
        
        

The result becomes

.. math::
        
        J_{i,j} = \frac{\partial F_i}{\partial c_j} =
        \int_\Omega  ({\psi}_j{\psi}_i + \Delta t\,{\alpha}^{\prime}(u){\psi}_j
        \nabla u\cdot\nabla {\psi}_i +
        \Delta t\,{\alpha}(u)\nabla{\psi}_j\cdot\nabla{\psi}_i - \nonumber
        

.. _Eq:nonlin:alglevel:dD:fe:Jij:

.. math::

    \tag{82}
    \ \Delta t f^{\prime}(u){\psi}_j{\psi}_i){\, \mathrm{d}x}{\thinspace .}
        
        

The evaluation of :math:`J_{i,j}` as the coefficient matrix in the linear
system in Newton's method applies the known approximation :math:`u^{-}`
for :math:`u`:

.. math::
        
        J_{i,j} =
        \int_\Omega  ({\psi}_j{\psi}_i + \Delta t\,{\alpha}^{\prime}(u^{-}){\psi}_j
        \nabla u^{-}\cdot\nabla {\psi}_i +
        \Delta t\,{\alpha}(u^{-})\nabla{\psi}_j\cdot\nabla{\psi}_i - \nonumber
        

.. _Eq:nonlin:alglevel:dD:fe:Jij2:

.. math::

    \tag{83}
    \ \Delta t f^{\prime}(u^{-}){\psi}_j{\psi}_i){\, \mathrm{d}x}{\thinspace .}
        
        

Hopefully, this example also shows how convenient the notation
with :math:`u` and :math:`u^{-}` is: the unknown to be computed is always :math:`u` and
linearization by inserting known (previously computed) values
is a matter of adding an underscore.
One can take great advantage of this quick notation in
software [Ref2]_.

Non-homogeneous Neumann conditions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A natural physical flux condition for the PDE :ref:`(75) <Eq:nonlin:alglevel:dD:fe:PDE>`
takes the form of a non-homogeneous Neumann condition

.. _Eq:nonlin:alglevel:dD:fe:Neumann:

.. math::

    \tag{84}
    -{\alpha}(u)\frac{\partial u}{\partial n} = g,\quad\boldsymbol{x}\in\partial\Omega_N,
        
        

where :math:`g` is a prescribed function and :math:`\partial\Omega_N` is a part
of the boundary of the domain :math:`\Omega`. From integrating
:math:`\int_\Omega\nabla\cdot({\alpha}\nabla u){\, \mathrm{d}x}` by parts, we get a boundary term

.. _Eq:nonlin:alglevel:dD:fe:boundary:integral:

.. math::

    \tag{85}
    \int_{\partial\Omega_N}{\alpha}(u)\frac{\partial u}{\partial u}v{\, \mathrm{d}s}{\thinspace .}
        
        

Inserting the condition :ref:`(84) <Eq:nonlin:alglevel:dD:fe:Neumann>` into
this term results in an integral over prescribed values:

.. math::
         -\int_{\partial\Omega_N}gv{\, \mathrm{d}s}{\thinspace .}

The nonlinearity in the :math:`{\alpha}(u)` coefficient condition
:ref:`(84) <Eq:nonlin:alglevel:dD:fe:Neumann>` therefore does not contribute with
a nonlinearity in the variational form.

Robin conditions
~~~~~~~~~~~~~~~~

Heat conduction problems often apply a kind of Newton's cooling law,
also known as a Robin condition, at the boundary:

.. _Eq:nonlin:alglevel:dD:fe:Robin:

.. math::

    \tag{86}
    -{\alpha}(u)\frac{\partial u}{\partial u} = h(u)(u-T_s(t)),\quad\boldsymbol{x}\in\partial\Omega_R,
        
        

where :math:`h(u)` is a heat transfer coefficient between the body (:math:`\Omega`)
and its surroundings, :math:`T_s` is the temperature of the surroundings,
and :math:`\partial\Omega_R` is a part of the boundary where this Robin
condition applies. The boundary integral
:ref:`(85) <Eq:nonlin:alglevel:dD:fe:boundary:integral>` now becomes

.. math::
         \int_{\partial\Omega_R}h(u)(u-T_s(T))v{\, \mathrm{d}s}{\thinspace .}

In many physical applications,
:math:`h(u)` can be taken as constant, and then the boundary
term is linear in :math:`u`, otherwise it is nonlinear and contributes
to the Jacobian in a Newton method.
Linearization in a Picard method will typically use a known value
in :math:`h`, but keep the :math:`u` in :math:`u-T_s` as unknown:
:math:`h(u^{-})(u-T_s(t))`. :ref:`nonlin:exer:dD:heat:nonlinear:c:a`
asks you to carry out the details.

.. _nonlin:alglevel:dD:fd:

Finite difference discretization          (2)
---------------------------------------------

A typical diffusion equation

.. math::
         u_t = \nabla\cdot({\alpha}(u)\nabla u) + f(u),

can be discretized by (e.g.) a Backward Euler scheme,
which in 2D can be written

.. math::
         [D_t^- u = D_x\overline{{\alpha}(u)}^xD_x u
        + D_y\overline{{\alpha}(u)}^yD_y u + f(u)]_{i,j}^n{\thinspace .}
        

We do not dive into the details of handling boundary conditions now.
Dirichlet and Neumann conditions are handled as in a
corresponding linear, variable-coefficient diffusion problems.

Writing the scheme out, putting the unknown values on the
left-hand side and known values on the right-hand side, and
introducing :math:`\Delta x=\Delta y=h` to save some writing, one gets

.. math::
        
        u^n_{i,j} &- \frac{\Delta t}{h^2}(
         \frac{1}{2}({\alpha}(u_{i,j}^n)   + {\alpha}(u_{i+1,j}^n))(u_{i+1,j}^n-u_{i,j}^n)\\ 
        &\quad -
        \frac{1}{2}({\alpha}(u_{i-1,j}^n) + {\alpha}(u_{i,j}^n))(u_{i,j}^n-u_{i-1,j}^n) \\ 
        &\quad +
         \frac{1}{2}({\alpha}(u_{i,j}^n)   + {\alpha}(u_{i,j+1}^n))(u_{i,j+1}^n-u_{i,j}^n)\\ 
        &\quad -
         \frac{1}{2}({\alpha}(u_{i,j-1}^n) + {\alpha}(u_{i,j}^n))(u_{i,j}^n-u_{i-1,j-1}^n))
        - \Delta tf(u_{i,j}^n) = u^{n-1}_{i,j}
        

This defines a nonlinear algebraic system on the form :math:`A(u)u=b(u)`.

Picard iteration          (4)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The most recently computed values :math:`u^{-}` of :math:`u^n` can be
used in :math:`{\alpha}` and :math:`f` for a Picard iteration,
or equivalently, we solve :math:`A(u^{-})u=b(u^{-})`.
The result is a linear system of the same type as arising
from :math:`u_t = \nabla\cdot ({\alpha}(\boldsymbol{x})\nabla u) + f(\boldsymbol{x},t)`.

The Picard iteration scheme can also be expressed in operator notation:

.. math::
         [D_t^- u = D_x\overline{{\alpha}(u^{-})}^xD_x u
        + D_y\overline{{\alpha}(u^{-})}^yD_y u + f(u^{-})]_{i,j}^n{\thinspace .}
        

Newton's method          (5)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

As always, Newton's method is technically more involved than
Picard iteration. We first define
the nonlinear algebraic equations to be solved, drop the
superscript :math:`n` (use :math:`u` for :math:`u^n`), and introduce :math:`u^{(1)}` for :math:`u^{n-1}`:

.. math::
        
        F_{i,j} &= u_{i,j} - \frac{\Delta t}{h^2}(\\ 
        &\qquad \frac{1}{2}({\alpha}(u_{i,j})   + {\alpha}(u_{i+1,j}))(u_{i+1,j}-u_{i,j}) -\\ 
        &\qquad \frac{1}{2}({\alpha}(u_{i-1,j}) + {\alpha}(u_{i,j}))(u_{i,j}-u_{i-1,j}) + \\ 
        &\qquad \frac{1}{2}({\alpha}(u_{i,j})   + {\alpha}(u_{i,j+1}))(u_{i,j+1}-u_{i,j}) -\\ 
        &\qquad \frac{1}{2}({\alpha}(u_{i,j-1}) + {\alpha}(u_{i,j}))(u_{i,j}-u_{i-1,j-1})) -
        \Delta t\, f(u_{i,j}) - u^{(1)}_{i,j} = 0{\thinspace .}
        

It is convenient to work with two indices :math:`i` and :math:`j` in 2D
finite difference discretizations, but it complicates
the derivation of the Jacobian, which then gets four indices.
(Make sure you really understand the 1D version of this problem
as treated in the section :ref:`nonlin:alglevel:1D:fd`.)
The left-hand expression of an equation :math:`F_{i,j}=0` is to be
differentiated with respect to each of the unknowns :math:`u_{r,s}`
(recall that this is short notation for :math:`u_{r,s}^n`), :math:`r\in{\mathcal{I}_x}`, :math:`s\in{\mathcal{I}_y}`:

.. math::
         J_{i,j,r,s} = \frac{\partial F_{i,j}}{\partial u_{r,s}}{\thinspace .} 

The Newton system to be solved in each iteration can be written as

.. math::
         \sum_{r\in{\mathcal{I}_x}}\sum_{s\in{\mathcal{I}_y}}J_{i,j,r,s}\delta u_{r,s} = -F_{i,j},
        \quad i\in{\mathcal{I}_x},\ j\in{\mathcal{I}_y}{\thinspace .}

Given :math:`i` and :math:`j`, only a few :math:`r` and :math:`s` indices give nonzero
contribution to the Jacobian since :math:`F_{i,j}` contains :math:`u_{i\pm 1,j}`,
:math:`u_{i,j\pm 1}`, and :math:`u_{i,j}`. This means that :math:`J_{i,j,r,s}` has
nonzero contributions only if :math:`r=i\pm 1`, :math:`s=j\pm 1`, as well as :math:`r=i`
and :math:`s=j`.  The corresponding terms in :math:`J_{i,j,r,s}` are
:math:`J_{i,j,i-1,j}`, :math:`J_{i,j,i+1,j}`, :math:`J_{i,j,i,j-1}`, :math:`J_{i,j,i,j+1}`,
and :math:`J_{i,j,i,j}`.  Therefore, the left-hand side of the Newton
system, :math:`\sum_r\sum_s J_{i,j,r,s}\delta u_{r,s}` collapses to

.. math::
        
         J_{i,j,r,s}\delta u_{r,s} =
        J_{i,j,i,j}\delta u_{i,j} & +
        J_{i,j,i-1,j}\delta u_{i-1,j} +
        J_{i,j,i+1,j}\delta u_{i+1,j} +
        J_{i,j,i,j-1}\delta u_{i,j-1}\\ 
        & +
        J_{i,j,i,j+1}\delta u_{i,j+1}
        

The specific derivatives become

.. math::
        
        J_{i,j,i-1,j} &= \frac{\partial F_{i,j}}{\partial u_{i-1,j}}\\ 
        &= \frac{\Delta t}{h^2}({\alpha}^{\prime}(u_{i-1,j})(u_{i,j}-u_{i-1,j})
        + {\alpha}(u_{i-1,j})(-1)),\\ 
        J_{i,j,i+1,j} &= \frac{\partial F_{i,j}}{\partial u_{i+1,j}}\\ 
        &= \frac{\Delta t}{h^2}(-{\alpha}^{\prime}(u_{i+1,j})(u_{i+1,j}-u_{i,j})
        - {\alpha}(u_{i-1,j})),\\ 
        J_{i,j,i,j-1} &= \frac{\partial F_{i,j}}{\partial u_{i,j-1}}\\ 
        &= \frac{\Delta t}{h^2}({\alpha}^{\prime}(u_{i,j-1})(u_{i,j}-u_{i,j-1})
        + {\alpha}(u_{i,j-1})(-1)),\\ 
        J_{i,j,i,j+1} &= \frac{\partial F_{i,j}}{\partial u_{i,j+1}}\\ 
        &= \frac{\Delta t}{h^2}(-{\alpha}^{\prime}(u_{i,j+1})(u_{i,j+1}-u_{i,j})
        - {\alpha}(u_{i,j-1})){\thinspace .}
        

The :math:`J_{i,j,i,j}` entry has a few more terms and is left as an
exercise.
Inserting the most recent approximation
:math:`u^{-}` for :math:`u` in the :math:`J` and :math:`F` formulas and then
forming :math:`J\delta u=-F` gives the linear system to be solved
in each Newton iteration. Boundary conditions will affect the
formulas when any of the indices coincide with a boundary value
of an index.

Continuation methods
--------------------

.. index:: continuation method

Picard iteration or Newton's method may diverge when solving PDEs with
severe nonlinearities. Relaxation with :math:`\omega <1`
may help, but in highly nonlinear problems it can be
necessary to introduce a *continuation parameter* :math:`\Lambda` in
the problem: :math:`\Lambda =0` gives a version of the
problem that is easy to solve, while
:math:`\Lambda =1` is the target problem. The idea is then
to increase :math:`\Lambda` in steps, :math:`\Lambda_0=0 ,\Lambda_1 <\cdots <\Lambda_n=1`,
and use the solution from the problem with :math:`\Lambda_{i-1}` as
initial guess for the iterations in the problem corresponding
to :math:`\Lambda_i`.

The continuation method is easiest to understand through an example.
Suppose we intend to solve

.. math::
         -\nabla\cdot\left( ||\nabla u||^q\nabla u\right) = f, 

which is an equation modeling the flow of a non-Newtonian fluid through
a channel or pipe. For :math:`q=0` we have the Poisson equation (corresponding
to a Newtonian fluid) and the problem is linear. A typical
value for pseudo-plastic fluids may be :math:`q_n=-0.8`. We can introduce
the continuation parameter :math:`\Lambda\in [0,1]` such that
:math:`q=q_n\Lambda`. Let :math:`\{\Lambda_\ell\}_{\ell=0}^n` be the sequence of
:math:`\Lambda` values in :math:`[0,1]`, with corresponding :math:`q` values
:math:`\{q_\ell\}_{\ell=0}^n`. We can then solve a sequence of problems

.. math::
        
        -\nabla\cdot\left( ||\nabla u^\ell||^q_\ell\nabla u^\ell\right) = f,\quad
        \ell = 0,\ldots,n,

where the initial guess for iterating on :math:`u^{\ell}` is the
previously computed solution :math:`u^{\ell-1}`. If a particular :math:`\Lambda_\ell`
leads to convergence problems, one may try a smaller
increase in :math:`\Lambda`:
:math:`\Lambda_* = \frac{1}{2} (\Lambda_{\ell-1}+\Lambda_\ell)`,
and repeat halving the step in :math:`\Lambda` until convergence is reestablished.

